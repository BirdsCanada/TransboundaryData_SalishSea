[["index.html", "Transboundary avian data for the Salish Sea Welcome data users!", " Transboundary avian data for the Salish Sea Author: Danielle Ethier and Dean Evans Version 4: November 2023 Welcome data users! The purpose of this workbook is to provide you with a detailed data overview and R script to compile transboundary sea duck and geospatial datasets for the Salish Sea. This step-by-step guide will walk you through each available data source, how to access the data, and how to compile it into a standardized data format. This resource will also give the user an overview of any limitation, gaps, or considerations when using various dataset. We hope you find this guide useful for you independent and collaborative research endeavors. Photo credit: The Oregonian "],["overview.html", "Chapter 1 Overview 1.1 Project Summary 1.2 Goals 1.3 Acknowledgements ", " Chapter 1 Overview Engaging a transboundary expert network to prioritize coastal and marine habitat management for sea ducks in the Salish Sea 1.1 Project Summary Sea ducks are an indicator of ecosystem health in the Salish Sea, which is of global importance to many populations of marine birds that use these transboundary waters throughout the annual cycle. It is therefore imperative to work across management jurisdictions to effectively conserve key areas of habitat for these birds. However, jurisdictional complexities often presents a major challenge to taking an ecosystem scale approach to sea duck management, not least because many information sources (i.e., biological datasets) end at the international border. This work aims to overcome this barrier by harmonizing information in a way that assists agencies responsible for implementing conservation prescriptions. 1.2 Goals This R user guide aims to fulfill one of the key project initiatives: to identify and assess compatibility of avian datasets in the Salish Sea. By the end of this guide, researchers or resource managers interesting in accessing and compiling the available data resources will know: Which transboundary data resources are available (2) Who the data owners are and how to access the data with appropriate permissions (3) How the datasets are structured, including details on the data collection protocols (4) What the limitations, challenges, and opportunities are associated with each dataset (5) How to compile the datasets into a standardized data format WARNING: This guide may not provide all the details needed for the specific analysis you are interested in doing. Data users are responsible for understanding the data collection protocols before proceeding. If in doubt, reach out to the data owner with your questions! 1.3 Acknowledgements A great deal of time and effort have gone into gathering, compiling, and making the data presented in this guide accessible. Data users need to appropriately acknowledge the source of the information they are using. This also should include an acknowledgement of volunteer efforts in instances where data were collected by citizen-scientist. This project was a collaborative initiative between Birds Canada, Environment and Climate Change Canada, Ducks Unlimited Canada, Pacific Habitat Join Venture, Washington Department of Fish and Wildlife, and Audubon Washington. Funding for this project was provided by the U.S. Fish and Wildlife Service through Funding Opportunity F23AS00018. This R User Guide was written by Dr.Â Danielle Ethier and Dean Evans with Birds Canada. Any comments, edits or suggestions can be sent to dethier@birdscanada.org. Your feedback is always welcome and appreciated! "],["AD2.html", "Chapter 2 Avian Data 2.1 Overview 2.2 BCCWS 2.3 BCMA 2.4 PSSS 2.5 PSEMP 2.6 eBird 2.7 CBC 2.8 Data Processing 2.9 Additional Avian Data Resources", " Chapter 2 Avian Data 2.1 Overview There are seemingly endless datasets that live on the hard drive of some retired government biologist computer. Since the goal of this guide to identify available data resources, we do not include datasets that are difficult to find, lack well documented metadata, or require complex data permissions. Thus, avian datasets needed to be openly accessible to be included in this guide. Generally, the datasets that are not openly accessible are older (pre-2000) and cover a narrow geographic extents. Because of this, the guide is scoped primarily to data collected post-2000. In this section of the guide we detail each avian dataset separately. The datasets are presented in the following order: Canada, U.S., transboundary. We then focus on how to compile the data into a standardized framework. 2.2 BCCWS British Columbia Coastal Waterbird Survey 2.2.1 Quick Data Overview Data British Columbia Coastal Waterbird Survey (BCCWS) Owner Birds Canada/ Canada Wildlife Service Status Active Years 1999 - present Seasons Monthly survey, with a winter focus from Sept - April Sampling Coastal surveys along designated routes Data Access Available directly in R, with permission from Birds Canada or through the NatureCounts webportal Contact rtorrenta@birdscanada.org 2.2.2 Data Collection Protocol BCCWS data collection protocol can be found online here. In short, surveys have been conducted by volunteers using a standardized protocol and data collection sheets. Shore-based counts are completed monthly on or near the second Sunday of each month from September to April. Surveys are complete within approximately 2 hours of high tide to maximize the opportunity for close observation. All waterbirds observed to a distance of 1 km from the high tide line are counted, except those that fly through without stopping. In the case of larger flocks, numbers are estimated by counting individuals and species in groups and scaling up (see Training Module for Volunteers). Data are entered through a customized online data entry system available on the Birds Canada website, NatureCounts. Observations are processed using the eBird data filters to flag rare species and high counts during observer data entry, and records are manually reviewed for form accuracy. 2.2.3 Avian Data Collected Observation counts of waterbirds and raptor seen during a survey are compiled at the scale of the route (i.e., the maximum count per species) on each monthly survey. These observations are divided into inland, near shore (shoreline to 500m out from high tide), off shore (beyond 500m), and total counts. The dataset is not zero-filled. Taxonomic Authority = eBird/Clements v2019 2.2.4 Auxiliary Data Collected Observer information: observer ID Survey information: time observation started, time observation ended, duration in hours Survey condition: precipitation, % cloud, sea condition, tide state, tide movement, visibility, survey equipment, human activity (all categorical) 2.2.5 Data Access, Permission, and Format Data can be freely accessed through the NatureCounts data download portal or directly through the naturecounts R package, which I will demonstrate later in this chapter. The BCCWS is Access Level 4 dataset, meaning a data request form must be submitted. This is not meant to be a barrier, rather a means of keeping track of who is using the data and for what purposes. Data are formatted using a standardized schema that is a core standard of the Avian Knowledge Network and which feeds into GBIF. This format is called the Bird Monitoring Data Exchange (BMDE), which includes 169 core fields for capturing all metric and descriptors associated with bird observations. 2.2.6 Data Use Considerations The data are collected using a standardize protocol, by trained citizen-science volunteers. This standardization is a strength of this datasets for making inferences about coastal waterbirds in the Canadian Salish Sea. The repeated sampling design of the BCCWS makes this dataset suitable for an occupancy modelling framework, in which the probability of detection can be modeled alongside occupancy. Auxiliary data collected during each survey are suitable for the detection process of the model. Using mean counts in abundance on a route within a year, these data have also been recently used to assess coastal waterbird trends. Measures of effort are innate to the dataset. Survey duration (column DurationinHours) and survey area can be used to make effort correction to counts. Survey areas for each survey route has been provided as a .shp file in the Data folder in this project directory. The number of years an observer has been doing the survey can also be used to correct for observer bias. There is spatial inbalance in the sampling design, with more routes occurring in the southern Salish Sea and around areas that are inhabited by people. Hard to access locations are not well sampled, nor are regions further to the north. Since this survey is shore-based, there will be a species sampling bias. Specifically, birds that use near shore habitats will be detected and counted more often than birds which use offshore habitats. This dataset may therefore be less suitable for modelling at-sea habitat use, for example. The dataset should be filtered prior to use. For example, rare species should be removed since they are not necessarily representative (e.g., those which occur on &lt;1% of routes). Depending on the analysis, routes that are run for only a short time (e.g., &lt;3 years) or which have incomplete data (e.g., &lt;8 months per year or &lt;4 core winter month) may be considered for removal.This dataframe is not zero-filled. It is up to the data user to zero-fill the martix prior to use. 2.2.7 Data Use Examples Ethier, D.M., P.J.A. Davidson, G. Sorenson, C. Jardine, D. Lepage, K. Barry, K. Devitt, D.W. Bradley. 2020. Twenty years of coastal waterbird trends suggest regional patterns of environmental pressure in British Columbia, Canada. Avian Conservation and Ecology. 15(2):20 Middleton, H. A., R. W. Butler, and P. Davidson. 2018. Waterbirds alter their distribution and behavior in the presence of Bald Eagles (Haliaeetus leucocephalus). Northwestern Naturalist 99:21â30. doi: 10.1898/nwn16-21.1. Crewe, T., K. Barry, P. Davidson, and D. Lepage. 2012. Coastal waterbird population trends in the Strait of Georgia 1999 â 2011: Results from the first 12 years of the British Columbia Coastal Waterbird Survey. British Columbia Birds 22: 8â35. Bower, J. L. 2009. Changes in marine bird abundance in the Salish Sea: 1975 to 2007. Marine Ornithology 37: 9â17. Badzinski, S. S., R. J. Cannings, T. E. Armenta, J. Komaromi, and P. J. Davidson. 2006. British Columbia Coastal Waterbird Survey: An evaluation of survey power and species trends after five years of monitoring. Canadian Wildlife Service, Pacific and Yukon Region, Technical Report Series no. 455: 1-122. 2.3 BCMA British Columbia Marine Bird Atlas 2.3.1 Quick Data Overview Data British Columbia Marine Bird Atlas (BCMA) Owner Birds Canada/ Pacific Wildlife Foundation Status Not Active Years 2008-2015 Seasons Monthly surveys over a full year, with some variability Sampling Vessel transects along designated routes, over different regions of the Canadian Salish Sea in each year Data Access Available directly in R or through the NatureCounts webportal Contact acouturier@bsc-eoc.org 2.3.2 Data Collection Protocol Protocols are published in technical reports (links below), which may differ slightly between years since each year the survey focused on a different geographic region within the Salish Sea. Also note that the year long survey period straddles a calendar year. In short, counts of birds and marine mammals were made from a small boat moving at 10-12 knots within a 200 m (100m on either side) wide transect parallel to the shoreline. Surveyors also recorded the number of birds and mammals seen beyond 100 m of the boat. Two observers scanned for birds and mammals on either side of the boat. One observer calls out waypoints approximately every 250-500 meters (depending on survey year), while the other observer recorded the data. Binoculars are used to assist in counting and identifying distant birds and mammals. In most situations, birds and mammals are counted individually. Flocks of more than about 100 individual birds were estimated by summing the number by groups of 10s of individuals. Technical reports can be found here: The Southern Gulf Islands (2008-2010) Burrard Inlet and Indian Arm (2011-2013) Boundary Bay (2007-2008) (no report available) Howe Sound (2014-2015) The Fraser River Estuary (2016-2017) 2.3.3 Avian Data Collected Counts of waterbird seen during a survey are compiled for each point along a survey transect (i.e., the maximum species count) on each monthly survey. These observations are divided into Port Section, Central Section, Board Section and total ObservationCount. The dataset is not zero-filled. Taxonomic Authority = AOU 2.3.4 Auxiliary Data Collected Observer information: observer ID Survey information: time observation started, time observation ended, duration in hours Survey condition: none recorded 2.3.5 Data Access, Permission, and Format Data can be freely accessed through the NatureCounts data download portal or directly through the naturecounts R package, which I will demonstrate later in this chapter. The BCMA is an Access Level 5 dataset, meaning a data request form is not required to get access to this dataset, and it can be openly downloaded directly into R using the naturecounts R package. Data are formatted using a standardized schema that is a core standard of the Avian Knowledge Network and which feeds into GBIF.This format is called the Bird Monitoring Data Exchange (BMDE), which includes 169 core fields used for capturing all metric and descriptors associated with bird observations. 2.3.6 Data Use Considerations Measures of effort are innate to the dataset. Survey duration (column DurationinHours) can be used to make effort correction to counts. Surveys targeted regions of the Canadian Salish Sea that are known to be important to seabirds. The sampling design is therefore not randomized with respect to habitat types samples, which may limit its usability for habitat suitability modelling. This dataframe is not zero-filled. It is up to the data user to zero-fill the martix prior to use. Some data are missing from the NatureCounts database. Birds Canada is working to get this updated before the official release of this book 2.3.7 Data Use Examples Davidson, P.J.A., R.J. Cannings, A.R. Couturier, D. Lepage, and C.M. Di Corrado (eds.). 2015. The Atlas of the Breeding Birds of British Columbia, 2008-2012. Bird Studies Canada, Delta, B.C. 2.4 PSSS Puget Sound Seabird Survey 2.4.1 Quick Data Overview Data Puget Sound Seabird Survey (PSSS) Owner Seattle Audubon/ Puget Sound Bird Observatory Status Active Years 2009 - present Seasons Monthly survey, with a winter focus from Oct - April Sampling Coastal surveys at designated points Data Access Available by contacting data owner Contact joshm@seattleaudubon.org 2.4.2 Data Collection Protocol PSSS data collection protocol can be found online here. In short, surveys are conducted by volunteers using a standardized protocol and data collection sheets. Shore-based counts are completed monthly on the first Saturday of each month from October to April. Surveys are completed within approximately 2 hours of high tide to maximize the opportunity for close observation. Surveys are a minimum of 15 minutes and a maximum of 30 minutes per site. All waterbirds observed to a distance of 300 m from the high tide line are counted, except those that fly through without stopping. For large flocks, surveys estimate both the min, max, and best estimate. Surveyors are required to attend a short training session with Seattle Audubon staff prior to their first survey. Data are entered through a customized online data entry system, available here. 2.4.3 Avian Data Collected Total observation counts of each waterbird species seen during a point survey are recorded, including bearing, distance, and sex ratio. Raptors are recorded separately from the other waterbird species. The dataset is not zero-filled. Taxonomic Authority = 2.4.4 Auxiliary Data Collected Observer information: observer name Survey information: time observation started, time observation ended Survey condition: weather, precipitation, sea state, tide movement, visibility, human activity, raptor activity (all categorical) 2.4.5 Data Access, Permission, and Format At the time of writing, the data were only accessible by reaching out to the Seattle Audubon directly and filling out a data share agreement. The data will be sent to you as a .xslx flat file which will be suitable for Data Processing. Ensure that you receive all the data for the specified temporal period you are interested in analyzing. This will be needed to allow for proper zero-filling. 2.4.6 Data Use Considerations The data are collected using a standardized protocol, by trained citizen-science volunteers. This standardization is a strength of this dataset for making inferences about coastal waterbirds in the US Salish Sea. Since surveyors gather information on distance and direction, estimates of bird density through distance sampling are possible. Specifically, detection of any species declines with the distance from the observer: poor sighting conditions, quality of observing equipment, and observer inexperience all contribute to declining detection likelihood as distance increases. Distance sampling provides a robust approach to estimating density and allows for the calculation of less biased density estimates. The repeated sampling design of the PSSS makes this dataset suitable for an occupancy modeling framework, in which the probability of detection can be modeled alongside occupancy. Auxiliary data collected during each survey are suitable for the detection process of the model. Measures of effort are innate to the dataset. Survey duration can be used to make effort corrections to counts. Is there a spatial imbalance in the sampling design? Since this survey is shore-based, there will be a species sampling bias. Specifically, birds that use nearshore habitats will be detected and counted more often than birds that use offshore habitats. This dataset may therefore be less suitable for modeling seabird habitat use, for example. This PSSS survey was designed to be similar to the BCCWS, with some notable differences: BCCWS PSSS Survey the second Sunday Survey the first Saturday Sept-April Oct-April 1km count distance 300m count distance Survey route Survey point 2.4.7 Data Use Examples Ward EJ, Marshall KN, Ross T, Sedgley A, Hass T, Pearson SF, Joyce G, Hamel NJ, Hodum PJ, Faucett R. 2015. Using citizen-science data to identify local hotspots of seabird occurrence. PeerJ 3:e704 https://doi.org/10.7717/peerj.704 2.5 PSEMP Midwinter Aerial Seabird Survey 2.5.1 Quick Data Overview Data Midwinter Aerial Seabird Survey (PSEMP) Owner Washington Department of Fish and Wildlife (WDFW) Status Active Years 1994 - present Seasons Early December until the surveys are complete, which is usually in February Sampling Aerial flights along fixed routes Data Access Open Data Portal Contact Joseph.Evenson@dfw.wa.gov and Kyle.Spragens@dfw.wa.gov 2.5.2 Data Collection Protocol Links to a data collection sheet and protocol if available Surveys are flown each year by a crew of experienced WDFW biologists from early December until the surveys are complete, which is usually by the end of January or February. The surveys were designed to occur during midwinter when sea ducks and other marine birds are least likely to be migrating. Flights are conducted at an altitude of 200 feet (61 meters) above the water and at an airspeed of 85-90 knots. Each biologist makes all marine bird and mammal observations within a 50-meter strip along their respective side of the aircraft. The surveys cover the entire shoreline of the inland marine waters of Washington. The offshore waters are sampled by flying a pre-designed zigzag pattern. PSEMP data processing notes can be found online by navigating to the ESRI open data portal. Download the data by clicking the Open button, then save the files to the Data directory in this project. Information on data processing can be found in the Midwinter Aerial Seabird Surveys Geoprocessing Metadata Notes.pdf. In short, during pre-processing the flight crew transcribe voice recordings of their observations, linking them with a series of point locations collected during the survey by GPS. The results are provided as two tab-delimited text files containing observation and track-log locations. When the surveyors have completed their transcription, a series of geoprocessing scripts are run which omit off-effort observations, calculate the extent of surveyed areas, and add appropriate attribute information to both observations and survey strips. 2.5.3 Avian Data Collected The PSEMP_Survey_Observations layer stores the processed bird observation data, including the total ObservationCount of species on a given transect. There are three additional tables that are used to assign species to species codes, scientific names, and groups. Species: This table lists the avian species groups recorded during the winter fixed-wing aerial shorebird surveys. It provides speciesâ common and scientific names as well as the species codes (TaxoNameID) necessary to crosswalk these species to group membership (for composite groups analyzed) and to other WDFW databases. PSEMP_Group: This table provides a list of composite groups of species surveys. Some of these groups were included in subsequent statistical analysis; these groups are noted in the âStatsRunâ attribute. SpeciesPSEMP_Group: This associative table allows species observations to be crosswalked to composite groups that they participate in.[^1] PSEMP_SpeciesObservation_Attribute.xlsx: This is an extra table that was attained from the PSEMP group directly. It contains descriptors of the PSEMP_Survey_Observations layer. A copy of this table is saved in the Data director. Taxonomic Authority - AOU with modifications 2.5.4 Auxiliary Data Collected Each observation point contains detailed attribute information including date, time, TransectID, ObserverID, and environmental characteristics (i.e., wind, glare). The MidwinterAerialSeaBirdSurvey.gdb also contains a PSEMP_Analysis_Strata polygon file that contains the spatial extent and boundaries of the basin-depth strata. There are nine basins and six depth classes identified. 2.5.5 Data Access, Permission, and Format Data are accessible online by navigating to the ESRI open data portal. Download the data by clicking the Open button. The data are in a geodatabase (.gdb), which is a proprietary format of ESRI meaning the data are not suited for exchange with other applications. My suggestion is that you import the data layers into ArcGIS, and export the PSEMP_SurveyObservations and Species tables as .txt files for import into Excel. Then they can be opened and saved as .csv files for processing in the Data directory of this project. Although the data are openly available, WDFW staff ask you develop a data-sharing agreement prior to analysis and publication. 2.5.6 Data Use Considerations Before using the information, please consult Joseph.Evenson@dfw.wa.gov or Kyle.Spragens@dfw.wa.gov. Appropriate use of the data or analysis requires a detailed understanding of the survey effort and how that effort has changed over time. Not accounting for these details is very likely to result in inappropriate interpretation. 2.5.7 Data Use Examples Michel, N., T. Bayard, A. Summers, G. Slater, and K. Spragens. 2021. Avian Habitat Suitability Models for Puget Sound Estuary Birds. Prepared for the Puget Sound Ecosystem Monitoring Program, Puget Sound Partnership. Tacoma, WA. 2.6 eBird eBird 2.6.1 Quick Data Overview Data eBird/ eBird Canada Owner Cornell/ Birds Canada Status Active Years 2002-present Seasons All year Sampling Checklist: stationary, traveling, incidental, historic Data Access Online access through Cornell or NatureCounts Contact ebird@cornell.edu 2.6.2 Data Collection Protocol eBird is a checklist based citizen-science program. There are three main data collection protocols, including: Stationary: objective is birding, a single fixed location, with specified start and end time Travelling: objective is birding, specified start and end time, and known distance of travel Incidental: objective is not on birding, but species was identified and submitted. Not a complete checklist. There are also several specialized protocols: Pelagic: applies to checklists that are made farther than two miles offshore on oceans, seas, or large lakes. These data are likely most valuable to this initiative. Banding: birds observed because they were captured for banding. Detection rates from banding are very different from normal birding Nocturnal Flight Call (NFC): nighttime contact calls of migrating songbirds as they pass through on their way to breeding grounds 2.6.3 Avian Data Collected eBird data are collected and organized around the concept of a checklist, representing observations from a single birding event, such as a 1 km walk through a park or 15 minutes observing bird feeders in a backyard. Each checklist contains a list of species observed and the counts of the number of individuals seen of each species. Taxonomic Authority = eBird/Clements v2019 2.6.4 Auxiliary Data Collected Observer information: observer ID Survey information: time of day, checklist duration, distance traveled, and number of observers. 2.6.5 Data Access, Permission, and Format eBird offers free access to its data via prepackaged and customizable file downloads and APIs. For all downloads and APIs, eBird require first that you submit a request with a short abstract so they can learn more about how eBird data are being used. eBird date requests can be made to Cornell here. There is also an option to request eBird Canada data through the NatureCounts platform by making a request here. However, for the purposes of this transboundary project, the data example will be using eBird data accessed through Cornell. eBird Basic Dataset (EBD) is the core dataset for accessing all raw eBird observations and associated metadata.There are several R packages available for summarizing data, including one that is managed by the Cornell Lab specifically for working with the EBD dataset: auk: eBird Data Extraction and Processing with AWK. eBird Observational Dataset (EOD) is made available through GBIF and includes species, date, and location. Additional metadata associated with these observations, including sampling event data (i.e., effort), are not included through this platform. eBirdâs Terms of Use are clearly outlined on their webpage, and should be reviewed prior to requesting data. eBird data are not zero-filled. Instructions on zero-filling the dataset can be found here. Additional information on the metadata strucutre can be found in the âeBird Basic Dataset Metadata (v1.14)â pdf, which is saved in the Data folder of this directory. 2.6.6 Data Use Considerations eBird is a semi-structured project, having flexible, easy to follow protocols that attract many participants, making it the largest avian citizen science dataset globally. However, despite the strengths of eBirdâs size, species observations collected through citizen science projects present a number of challenges that are not found in conventional scientific data. The following are some of the primary challenges associated these data: taxonomic, spatial, and temporal biases, spatial precision, class inbalance, and variation in detection. When working with semi-structured datasets like eBird, one approach to dealing with this variation is to impose some more consistent structure on the data by filtering observations on the effort variables. This reduces the variation in detectability between checklists. It is generally recommended that users restricting checklists to less than 5 hours long and 5 km in length, and with 10 or fewer observers. Users of eBird data should review and adopt Best Practices for Using eBird Data. Recommended Citation: eBird Basic Dataset. Version: EBD_relSep-2022. Cornell Lab of Ornithology, Ithaca, New York. Month Year of Access. 2.6.7 Data Use Examples An exhaustive list of eBird publications can be found online here. 2.7 CBC Christmas Bird Count 2.7.1 Quick Data Overview Data Christmas Bird Count (CBC) Owner Audubon Status Active Years 1901 - present Seasons Between December 14 - January 5: 1 day count Sampling All winter bird in a specified area Data Access Available by contacting data owner for large downloads Contact cbcadmin@audubon.org. 2.7.2 Data Collection Protocol Winter bird are counted on a single calendar day between December 14âJanuary 5. Count volunteers follow specified routes through a designated 15-mile (24-km) diameter circle, counting every bird they see or hear all day. Participants join groups that survey sub-units of the circle during the course of the day using a variety of transportation methods (mostly on foot, in a car, or watching at a feeder, but can include boat, ski, or snowmobile). All birds seen or heard are recorded. Data compilers for each circle collect and submit data to Audubon. 2.7.3 Avian Data Collected All birds are counted all day, giving an indication of the total number of birds in the circle that day. CBC data are entered online through a program-specific webportal. Data can also be submitted using the eBird mobile app. 2.7.4 Auxiliary Data Collected Observer information: number of individuals participating including the compilers ID Survey information: duration of counts, number of party-hours, types of counts (e.g., road, walk), and distance travelled Weather information: temperature, wind, snow, cloud, rain 2.7.5 Data Access, Permission, and Format Christmas Bird Count (CBC) is a National Audubon Society owned and operated citizen science and research program and database. The CBC Database is housed on Audubon-owned servers and is accessible only through the Audubon web portal. Terms of Use govern access to and use of the CBC Database. Please read these terms carefully. By accessing the CBC Database, you agree to be bound by these Terms and by Audubonâs policy Privacy Policy. Larger data subsets of raw data may be provided for analytical use. To request permission, contact Audubon at cbcadmin@audubon.org. A online form will be send to you to complete. When requesting data from Audubon, keep in mind that you do not need all the available data. You should make a request that matched the spatial and temporal extend of the study to limit the file size. For example, my request included Washington State and British Columbia, and the years 1990-current. You will also be able to specify the data tables you require. In this instance, request all the available data tables (i.e., counts, effort, weather, and lat/longs). You must credit Audubon and the volunteers who collected the data in all published material containing CBC data, including scientific papers, articles, adverts, talks and slides with the following: âCBC Data are provided by National Audubon Society and through the generous efforts of Bird Studies Canada and countless volunteers across the Western Hemisphere.â Where possible, you should provide a link to the Audubon and CBC websites and, when using CBC data collected in Canada, Birds Canada, Audubonâs Canadian CBC partner. Please review the CBC data use policy prior to requesting data. 2.7.6 Data Use Considerations The Christmas Bird Count has undoubted value for documenting broad patterns of change in winter distribution (both annual and long-term) and for studies of patterns in species richness. No other data set provides such broad temporal and geographic coverage of North Americaâs winter bird life! However, like other citizen-science programs, there are several date use considerations to keep in mind. Although the number of uncommon species detected on a given count is certain to increase with effort, rare species are often scouted ahead of time, and the emphasis that most participants put on high species totals ensures that the distributional, presenceâabsence data in the CBC are very good. Counts within and between circles can vary because of sampling biases, including number of participants, hours in the field, extent and modes of travel, cover-age of different habitats, skill levels, and use of attractive devices (e.g.Â bird feeders, noises such as âpishingâ). Some of these biases can be accounted for in an analysis if the necessariy data are recorded. For example, survey effort has changes over the years. In recent decades, the number of party-hours has been recorded to account for the variable duration of, and participation in, the count. Thus, party-hour is often considered as an effort covariate in model development. Spatial distribution of count circles is not random. Like many citizen science programs, count sites were originally placed near population centers. This spatial bias towards urban center still persists. Finally, CBC is a land-based survey. The focus of data collection is therefore not on coastal waterbirds or seabirds. 2.7.7 Data Use Examples Meehan, T.D., Michel, N.L. and Rue, H. 2019. Spatial modeling of Audubon Christmas Bird Counts reveals fineâscale patterns and drivers of relative abundance trends. Ecosphere 10: e02707. Portland Christmas Bird Count â An 80-year review of species trends (1938-2017) 2.8 Data Processing Before continuing with this chapter, please review the content in Chapters 2-7. It is the responsibility of the data user to understand the various data collection protocols, stipulations around data access, and data use considerations. Start by loading the packages needed for this chapter #install.packages(&quot;remotes&quot;) #remotes::install_github(&quot;BirdsCanada/naturecounts&quot;) library(naturecounts) library(tidyverse) library(stringr) library(auk) library(measurements) library(reshape2) library(reshape) 2.8.1 Data Schema The purpose of this chapter is to provide the data user R script which will enable the compilation of disparate avian data sources into a standardized format (also known as a schema). The format selected for the purposes of this project was the Bird Monitoring Data Exchange (BMDE), which is the core standard of NatureCounts a node of the Avian Knowledge Network. The BMDE includes 169 core fields for capturing all metric and descriptors associated with bird observations. You can use the naturecounts R package and the following scripts to view the BMDE core fields. A copy of the BMDE table is also in the BMDE folder of this directory, which provides additional descriptions of the core columns. BMDE&lt;-meta_bmde_fields(&quot;core&quot;) Any data owners wishing to contribute their data to the NatureCounts database should complete the metadata form, also found in the BMDE folder, and reach out to Catherine Jardin: cjardine@birdscanada.org, Birds Canadaâs Data Analyst. 2.8.2 Species Codes A crucial steps to combining datasets is the inclusion of a common species code. Letâs compile the complete species list now for use later in the data compilation. The data tables you will need can be accessed using the naturecounts R package. sp.code&lt;-meta_species_codes() sp.code&lt;-sp.code %&gt;% filter(authority==&quot;BSCDATA&quot;) %&gt;% select(-authority, -species_id2, -rank) %&gt;% distinct() sp.tax&lt;-meta_species_taxonomy() sp.tax&lt;-sp.tax %&gt;% select(species_id, scientific_name, english_name) %&gt;% distinct() sp&lt;-left_join(sp.code, sp.tax, by=&quot;species_id&quot;) sp&lt;-sp %&gt;% distinct(english_name, .keep_all = TRUE) 2.8.3 Data Manipulation All data sets need to be manipulated before they are used for an analysis. Anyone that uses big datasets for research will tell you that it often takes more time to manipulate and filter the data than doing the actual statistical analysis. I therefore cannot cover all the possible data manipulations you will do, but will give you some sample script to help compile a contemporary dataset of birds using the tansboundary waters of the Salish Sea. To accommodate the R scripts in this chapter, data samples for each dataset are provided in the Data folder. These samples are structured in the same format you will receive the raw data from the data owner. There are data column in the raw dataset that are not carried over to the BMDE. We do retain any information which could be used for effort correction, but some auxiliary data is lost which could bee used if developing a occupancy model. If you are a data user, you should inspect the raw data columns to ensure there is nothing that you need dropped from the final datatable. You can change the code below to retain additional columns as needed. 2.8.4 BCCWS &amp; BCMA We start with two Canadian datasets that are already in the BMDE schema and accessible through NatureCounts. The BCCWS and BCMA are accessible through the NatureCounts web portal or directly using the naturecounts R package. Since the BCCWS is Access Level 3, you need to make a data request. Once your NatureCounts data request has been approved you will receive an email confirmation, which will contain your request_id. This number will be used to download your newly acquired dataset into R. The BCMA is open access, Level 5, and therefore a data request_id is not required. If you are new to the naturecounts R package, I recommend you start by reviewing the Introductory R Tutorial. Those materials will not be repeated here. Below is some sample code for downloading the BCCWS and BCMA datasets. You will replace the request_id and username with your own credentials. To retrieve the core BMDE columns, you will want the fields_set to be set to âcoreâ (as below). library(naturecounts) BCCWS&lt;-nc_data_dl(collection=&quot;BCCWS&quot;, username = &quot;YOUR USERNAME&quot;, info=&quot;MY REASON&quot;, fields_set = &quot;core&quot;) BCMA&lt;-nc_data_dl(collection=&quot;BCMA&quot;, username = &quot;YOUR USERNAME&quot;, info=&quot;MY REASON&quot;, fields_set = &quot;core&quot;) Working with the sample datasets. BCCWS&lt;-read.csv(&quot;Data/BCCWS_sample.csv&quot;) BCMA&lt;-read.csv(&quot;Data/BCMA_sample.csv&quot;) #select only core BMDE columns, since they seem to vary library(naturecounts) BMDE&lt;-meta_bmde_fields(&quot;core&quot;) BMDE_col&lt;-unique(BMDE$local_name) BCCWS&lt;-BCCWS %&gt;% select(all_of(BMDE_col)) BCMA&lt;-BCMA %&gt;% select(all_of(BMDE_col)) #There seems to be some duplicates in the full BCCWS dataset that need removed. BCCWS&lt;-BCCWS %&gt;% distinct(RouteIdentifier, SpeciesCode, YearCollected, MonthCollected, DayCollected, DecimalLatitude, DecimalLongitude, .keep_all=TRUE) #Assign SurveyAreaIdentifier to RouteIdentifier for BCMA BCMA$RouteIdentifier&lt;-BCMA$SurveyAreaIdentifier 2.8.5 PSSS The data will be received in an .xlsx file. Save as a .csv in the Data folder for processing using the following scripts. We will work with the sample dataset here which has the same formatting as the full dataset that you will receive. Note: the sample dataframe is 50 rows long, but the output dataframe will be 65. How can this be!? This is because the raptors are recorded in rows with the other data and are extracted into their own rows to match the BMDE. This makes the data frame longer. PSSS&lt;-read.csv(&quot;Data/PSSS_sample.csv&quot;) PSSS$lat&lt;-sub(&quot; W.*&quot;, &quot;&quot;, PSSS$position) PSSS$long&lt;-sub(&quot;.*W&quot;, &quot;&quot;, PSSS$position) PSSS$lat = gsub(&#39;N&#39;, &#39;&#39;, PSSS$lat) PSSS$long = gsub(&#39;W&#39;, &#39;&#39;, PSSS$long) PSSS$DecimalLatitude = measurements::conv_unit(PSSS$lat, from = &#39;deg_dec_min&#39;, to = &#39;dec_deg&#39;) PSSS$DecimalLatitude&lt;-as.numeric((PSSS$DecimalLatitude)) PSSS$DecimalLongitude = measurements::conv_unit(PSSS$long, from = &#39;deg_dec_min&#39;, to = &#39;dec_deg&#39;) PSSS$DecimalLongitude&lt;-as.numeric(PSSS$DecimalLongitude) PSSS$DecimalLongitude=PSSS$DecimalLongitude*(-1) #break apart survey_date and reform into day, month, year PSSS&lt;-PSSS %&gt;% separate(survey_date, into=c(&quot;Date&quot;, &quot;del&quot;), sep=&quot; &quot;) %&gt;% select(-del) %&gt;% separate(Date, into=c(&quot;YearCollected&quot;, &quot;MonthCollected&quot;, &quot;DayCollected&quot;), sep=&quot;-&quot;) #wrangle raptor data into the long format since each species identification should be in a unique row. raptor1&lt;-PSSS %&gt;% filter(raptor1 != &quot;&quot;) %&gt;% mutate(common_name = raptor1, bird_count = raptor1_count, notes= raptor1_affect)%&gt;% select(-raptor1, -raptor2, -raptor3, -raptor1_count, -raptor2_count, -raptor3_count, -raptor1_affect, -raptor2_affect, -raptor3_affect) raptor1&lt;-raptor1 %&gt;% group_by(site_name, common_name, YearCollected, MonthCollected, DayCollected) %&gt;% mutate(bird_count=sum(bird_count)) %&gt;% distinct(common_name, site_name, YearCollected, MonthCollected, DayCollected, .keep_all=TRUE) raptor2&lt;-PSSS %&gt;% filter(raptor2 != &quot;&quot;) %&gt;% mutate(common_name = raptor2, bird_count = raptor2_count, notes= raptor2_affect)%&gt;% select(-raptor1, -raptor2, -raptor3, -raptor1_count, -raptor2_count, -raptor3_count, -raptor1_affect, -raptor2_affect, -raptor3_affect) raptor2&lt;-raptor2 %&gt;% group_by(site_name, common_name, YearCollected, MonthCollected, DayCollected) %&gt;% mutate(bird_count=sum(bird_count)) %&gt;% distinct(common_name, site_name, YearCollected, MonthCollected, DayCollected, .keep_all=TRUE) raptor3&lt;-PSSS %&gt;% filter(raptor3 != &quot;&quot;) %&gt;% mutate(common_name = raptor3, bird_count = raptor3_count, notes= raptor3_affect) %&gt;% select(-raptor1, -raptor2, -raptor3, -raptor1_count, -raptor2_count, -raptor3_count, -raptor1_affect, -raptor2_affect, -raptor3_affect) raptor3&lt;-raptor3 %&gt;% group_by(site_name, common_name, YearCollected, MonthCollected, DayCollected) %&gt;% mutate(bird_count=sum(bird_count)) %&gt;% distinct(common_name, site_name, YearCollected, MonthCollected, DayCollected, .keep_all=TRUE) PSSS&lt;-PSSS %&gt;% select(-raptor1, -raptor2, -raptor3, -raptor1_count, -raptor2_count, -raptor3_count, -raptor1_affect, -raptor2_affect, -raptor3_affect) #bind raptor data back with PSSS data PSSS&lt;-rbind(PSSS, raptor1) PSSS&lt;-rbind(PSSS, raptor2) PSSS&lt;-rbind(PSSS, raptor3) #remove rows with missing common name PSSS&lt;-PSSS %&gt;% filter(common_name !=&quot;&quot;) #remove bearing and distance because we want each species/ site/ date to be a single row in the data set similar to BBCWS PSSS&lt;-PSSS %&gt;% select(-bearing, -dist) #Now summarize the records per species/ site/ date PSSS&lt;-PSSS %&gt;% group_by(site_name, common_name, YearCollected, MonthCollected, DayCollected) %&gt;% mutate(bird_count=sum(bird_count)) %&gt;% distinct(common_name, site_name, YearCollected, MonthCollected, DayCollected, .keep_all=TRUE) #replace Thayer&#39;s Gull with Ivory Gull PSSS&lt;-PSSS %&gt;% mutate(common_name = ifelse(common_name == &quot;Thayer&#39;s Gull&quot;, &quot;Ivory Gull&quot;, common_name)) #Merge with species ID PSSS&lt;-merge(PSSS, sp, by.x=c(&quot;common_name&quot;), by.y= (&quot;english_name&quot;), all.x=TRUE) #rename data columns to match BMDE PSSS&lt;-PSSS %&gt;% dplyr::rename(CommonName =common_name, SurveyAreaIdentifier= survey_site_id, Locality = site_name, MinimumElevationInMeters=elevation, MaximumElevationInMeters=elevation, TimeObservationsStarted=start_time, TimeCollected = start_time, TimeObservationsEnded=end_time, ObservationCount = bird_count, ObservationCount2=large_flock_best, ObsCountAtLeast = large_flock_min, ObsCountAtMost = large_flock_max, FieldNotes=notes, Collector = name, ScientificName=scientific_name, SpeciesCode=species_code, AllSpeciesReported=is_complete) PSSS$RouteIdentifier&lt;-PSSS$Locality PSSS$BasisOfRecord &lt;- &quot;Observation&quot; PSSS$CollectionCode &lt;- &quot;PSSS&quot; PSSS$Continent &lt;-&quot;North America&quot; PSSS$Country&lt;-&quot;United States&quot; PSSS$StateProvince&lt;-&quot;Washington&quot; PSSS$ProtocolType &lt;- &quot;PointCount&quot; PSSS$ProtocolSpeciesTargeted &lt;- &quot;Waterbirds&quot; PSSS$ProtocolURL= &quot;https://seattleaudubon.org/wp-content/uploads/2021/01/PSSS_Protocol_2014-15.pdf&quot; PSSS$SurveyAreaShape = &quot;300 m&quot; #PSSS$EffortUnit1 = &quot;Party-hours&quot; PSSS$ObservationDescriptor = &quot;Total Count&quot; PSSS$ObservationDescriptor2 = &quot;Large flock best estiamte&quot; #Now that we have specified all the data columns we can, we will create the BMDE standardized data table. #Identify the missing columns of data BMDE_col&lt;-unique(BMDE$local_name) missing&lt;-setdiff(BMDE_col, names(PSSS)) PSSS[missing]&lt;-&quot; &quot; PSSS&lt;-PSSS[BMDE_col] 2.8.6 PSEMP The data will be downloaded as a .gdb file, which is a proprietary format of ESRI meaning the data are not suited for exchange with other applications. My suggestion is that you (or your GIS analyst) import the data layers into ArcGIS, and export tables in .txt. format for import into Excel. Then save as .csv in the Data directory of this project. The tables provided will include: PSEMP_Survey_Observations (PSEMP_sample.csv): This is a point layer storing processed bird observation data recorded during the winter fixed-wing aerial shorebird surveys. There are two observers in the plane, one on each side facing outward, that record their observations into an audio recording device along with timestamp information and position data from an on-board GPS receiver. This data is later transcribed into tab-delimited files along with a timestamped track-log for the plane (also tab-delimited). This source data are then imported into GIS format and a series of geoprocessing scripts are run which project these points spatially 63 meters at right angles to either side of the plane as well as filtering them (to reduce the chance of double counting) and adding additional attribute information. You will want to add the X and Y coordinates to this table before you export it from ArcGIS. If you are not familiar with how to do this, you will find some useful instructions here. PSEMP_SurveyRoutes: This is a polyline GIS data layer representing the flight path of the fixed wing plane recorded (initially as a series of point locations) during the winter fixed-wing aerial shorebird surveys. PSEMP_SurveyArea: This is a polygon data layer that represents the area considered to be visually surveyed during the annual winter fixed-wing aerial shorebird surveys. The data consist of rectangular strips that run parallel to the flight path of the plane. The strips are 50 meters wide and are centered 63 meters to either side of the plane. Locations of shorebird observations recorded along the flight path are spatially projected into the center of these strips. The purpose of the strips is to provide an estimate of surveyed area as well as serving as inputs in a process wherein some shorebird observations are removed from areas of survey strip overlap in an effort to minimize the chances of double counting birds. Species (PSEMP_species.csv): This table lists the avian species groups recorded during the winter fixed-wing aerial shorebird surveys. It provides species common and scientific names as well as the species codes (TaxoNameID) necessary to crosswalk these species to group membership (for composite groups analyzed) and to other WDFW databases. PSEMP_Group: This table provides a list of composite groups of species surveys. Some of these groups were included in subsequent statistical analysis; these groups are noted in the âStatsRunâ attribute. SpeciesPSEMP_Group: This associative table allows species observations to be crosswalked to composite groups that they participate in. PSEMP_SpeciesObservation_Attribute.xlsx: This is an extra table that was attained from the PSEMP group directly. It contains descriptors of the PSEMP_Survey_Observations layer. A copy of this table is saved in the Data director. We will work with the sample dataset here which has the same formatting as the full dataset that you will receive through the online download. PSEMP&lt;-read.csv(&quot;Data/PSEMP_sample.csv&quot;) #note the x and y were added in ArcGIS before export. #Get species code information from the NatureCounts R package (duplicate scripts) sp.code&lt;-meta_species_codes() sp.code&lt;-sp.code %&gt;% filter(authority==&quot;BSCDATA&quot;) %&gt;% select(-authority, -species_id2, -rank) %&gt;% distinct() sp.tax&lt;-meta_species_taxonomy() sp.tax&lt;-sp.tax %&gt;% select(species_id, scientific_name, english_name) %&gt;% distinct() sp&lt;-left_join(sp.code, sp.tax, by=&quot;species_id&quot;) sp&lt;-sp %&gt;% distinct(english_name, .keep_all = TRUE) #load the PSEMP species table sp_PSEMP&lt;-read.csv(&quot;Data/PSEMP_species.csv&quot;) sp_PSEMP&lt;-sp_PSEMP %&gt;% select(TaxoNameID, PSEMP_CommonName, PSEMP_SpeciesCode, PSEMP_SciName1) %&gt;% distinct() %&gt;% filter(TaxoNameID&gt;=1) #some species codes need changed in order to properly link this with the sp table from the NatureCounts database. sp_PSEMP&lt;-sp_PSEMP %&gt;% mutate(PSEMP_SpeciesCode=ifelse(PSEMP_CommonName==&quot;Cormorant, Brandt&#39;s&quot;, &quot;BRAC&quot;, PSEMP_SpeciesCode)) %&gt;% mutate(PSEMP_SpeciesCode=ifelse(PSEMP_CommonName==&quot;Plover, American golden&quot;, &quot;AMGP&quot;, PSEMP_SpeciesCode)) %&gt;% mutate(PSEMP_SpeciesCode=ifelse(PSEMP_CommonName==&quot;Duck, harlequin&quot;, &quot;HARD&quot;, PSEMP_SpeciesCode)) %&gt;% mutate(PSEMP_SpeciesCode=ifelse(PSEMP_CommonName==&quot;Gull, herring&quot;, &quot;HERG&quot;, PSEMP_SpeciesCode)) %&gt;% mutate(PSEMP_SpeciesCode=ifelse(PSEMP_CommonName==&quot;Gull, Heermann&#39;s&quot;, &quot;HEEG&quot;, PSEMP_SpeciesCode)) %&gt;% mutate(PSEMP_SpeciesCode=ifelse(PSEMP_CommonName==&quot;Shoveler, northern&quot;, &quot;NSHO&quot;, PSEMP_SpeciesCode)) %&gt;% mutate(PSEMP_SpeciesCode=ifelse(PSEMP_CommonName==&quot;Crow, northwestern&quot;, &quot;NOCR&quot;, PSEMP_SpeciesCode)) %&gt;% mutate(PSEMP_SpeciesCode=ifelse(PSEMP_CommonName==&quot;Hawk, red-tailed&quot;, &quot;HAHA&quot;, PSEMP_SpeciesCode)) %&gt;% mutate(PSEMP_SpeciesCode=ifelse(PSEMP_CommonName==&quot;Gull, Thayer&#39;s&quot;, &quot;ICGU&quot;, PSEMP_SpeciesCode)) %&gt;% mutate(PSEMP_SpeciesCode=ifelse(PSEMP_CommonName==&quot;Swan, trumpeter&quot;, &quot;TRUS&quot;, PSEMP_SpeciesCode)) %&gt;% mutate(PSEMP_SpeciesCode=ifelse(PSEMP_CommonName==&quot;Goose, white-fronted&quot;, &quot;GWFG&quot;, PSEMP_SpeciesCode)) %&gt;% mutate(PSEMP_SpeciesCode=ifelse(PSEMP_CommonName==&quot;Pelican, American white&quot;, &quot;AWPE&quot;, PSEMP_SpeciesCode)) %&gt;% filter(PSEMP_SpeciesCode!=&quot;HAPO&quot;) %&gt;% select(-TaxoNameID, -PSEMP_CommonName, -PSEMP_SciName1) #join species tables sp_PSEMP&lt;-merge(sp_PSEMP, sp, by.x=&quot;PSEMP_SpeciesCode&quot;, by.y=&quot;species_code&quot;, all.x=TRUE) #join to observation data PSEMP&lt;-left_join(PSEMP, sp_PSEMP, by=&quot;PSEMP_SpeciesCode&quot;) #remove any species that start with &#39;U&#39; as unidentified PSEMP&lt;-PSEMP %&gt;% filter(!is.na(species_id)) #join to the area table area_PSEMP&lt;-read.csv(&quot;Data/PSEMP_SurveyArea.csv&quot;) area_PSEMP&lt;-area_PSEMP %&gt;% select(TransectID, SurveyYear, Shape_Length, Shape_Area) area_PSEMP&lt;-area_PSEMP %&gt;% distinct(TransectID, SurveyYear, .keep_all = TRUE) PSEMP&lt;-left_join(PSEMP, area_PSEMP, by=c(&quot;TransectID&quot;, &quot;SurveyYear&quot;)) #Separate data and time field into separate columns PSEMP$DateTimeFromLog&lt;-as.Date(PSEMP$ObservationDateTime) PSEMP&lt;-PSEMP %&gt;% separate(ObservationDateTime, c(&quot;YearCollected&quot;, &quot;MonthCollected&quot;, &quot;del&quot;), sep=&quot;-&quot;) PSEMP&lt;-PSEMP %&gt;% separate(del, c(&quot;DayCollected&quot;, &quot;TimeCollected&quot;), sep=&quot; &quot;) #rename columns to match BMDE PSEMP&lt;-PSEMP %&gt;% dplyr::rename(RouteIdentifier=TransectID, ProtocolType=TransectType, CollectorNumber = ObserverID, SamplingEventIdentifier=ObservationID, BearingInDegrees=ObservationDegreesDTrue, DecimalLatitude=Y, DecimalLongitude=X, ScientificName = scientific_name, SpeciesCode=PSEMP_SpeciesCode, CommonName=english_name, SurveyAreaSize=Shape_Area, Remarks=ObservationComment) #1-Observation on shoreline transect; 2-Observation on open water transect; Unsure why some years are Null PSEMP&lt;-PSEMP %&gt;% mutate(ProtocolType=ifelse(ProtocolType==1, &quot;Shoreline Transect&quot;, ifelse(ProtocolType==2, &quot;Open Water Transect&quot;, &quot;Null&quot;))) PSEMP$Locality&lt;-PSEMP$RouteIdentifier PSEMP$BasisOfRecord &lt;- &quot;Observation&quot; PSEMP$CollectionCode &lt;- &quot;PSEMP&quot; PSEMP$Continent &lt;-&quot;North America&quot; PSEMP$Country&lt;-&quot;United States&quot; PSEMP$StateProvince&lt;-&quot;Washington&quot; PSEMP$ProtocolSpeciesTargeted &lt;- &quot;Waterbirds&quot; PSEMP$InstitutionCode&lt;-&quot;PSEMP&quot; PSEMP$NumberOfObservers&lt;-2 #Remove non-unique observations and sum results per location/ date (double observer) PSEMP&lt;-PSEMP %&gt;% group_by(RouteIdentifier, SpeciesCode, YearCollected, MonthCollected, DayCollected, DecimalLatitude, DecimalLongitude) %&gt;% mutate(ObservationCount=sum(ObservationCount)) %&gt;% distinct(RouteIdentifier, SpeciesCode, YearCollected, MonthCollected, DayCollected, DecimalLatitude, DecimalLongitude, .keep_all=TRUE) #Now that we have specified all the data columns we can, we will create the BMDE standardized data table. #Identify the missing columns of data BMDE_col&lt;-unique(BMDE$local_name) missing&lt;-setdiff(BMDE_col, names(PSEMP)) PSEMP[missing]&lt;-&quot; &quot; PSEMP&lt;-PSEMP[BMDE_col] 2.8.7 eBird You will make an online request for eBird data scoped to the geographic region and temporal scale of choice. In this instance, I would request all the data for Washington and British Columbia from 2002-present. Each region needs to be requested separately. Once you have received the raw data files, you can save them in the Data directory of this folder. They will be BIG! Due to the large size of this dataset, it must be filtered to a smaller subset of desired observations before reading into R. This filtering is most efficiently done using auk: eBird Data Extraction and Processing with AWK a Unix utility and programming language for processing column formatted text data. This package acts as a front end for AWK, allowing users to filter eBird data before import into R. There will be several files in the .zip folder along with your raw data, including: BCRCodes, IBACodes, USFWSCodes, recommend citation, terms of use, and the metadata file. We will once again work with the sample dataset here which has the same formatting as the full dataset that you will receive through the online download. #start by setting the working directory of the abd files to the data directory of this project, where your data files should be saved #getwd() # your current working directory #auk_set_ebd_path(&quot;C:/Users/dethier/Documents/ethier-scripts/DataCompile-SDJV/Data/&quot;, overwrite=FALSE) # my current working directory (sample code) WA_in&lt;-&quot;Data/eBirdWA_sample.txt&quot; WA_out&lt;-&quot;Data/WA_filter.txt&quot; ebird_WA&lt;-WA_in %&gt;% auk_ebd() %&gt;% #define filters auk_bcr(bcr=5) %&gt;% #auk_protocol(&quot;eBird Pelagic Protocol&quot;) %&gt;% auk_protocol(&quot;Stationary&quot;) %&gt;% auk_filter(file=WA_out, overwrite=TRUE) %&gt;% read_ebd() BC_in&lt;-&quot;Data/eBirdBC_sample.txt&quot; BC_out&lt;-&quot;Data/BC_filter.txt&quot; ebird_BC&lt;-BC_in %&gt;% auk_ebd() %&gt;% #define filters auk_bcr(bcr=5) %&gt;% #auk_protocol(&quot;eBird Pelagic Protocol&quot;) %&gt;% auk_protocol(&quot;Stationary&quot;) %&gt;% auk_filter(file=BC_out, overwrite=TRUE) %&gt;% auk_complete() %&gt;% read_ebd() ebird_data&lt;-rbind(ebird_BC, ebird_WA) #separate data and time columns ebird_data&lt;-ebird_data %&gt;% separate(observation_date, into=c(&quot;YearCollected&quot;, &quot;MonthCollected&quot;, &quot;DayCollected&quot;), sep=&quot;-&quot;) #rename columns ebird_data&lt;-ebird_data %&gt;% dplyr::rename (GlobalUniqueIdentifier=global_unique_identifier, DateLastModified=last_edited_date, TaxonConceptID=taxon_concept_id, CommonName=common_name, ScientificName=scientific_name, ObservationCount=observation_count, Country=country_code, StateProvince=state, SurveyAreaIdentifier=locality_id, TimeCollected=time_observations_started, CollectorNumber=observer_id, SamplingEventIdentifier=sampling_event_identifier, ProtocolType=protocol_type, ProtocolCode=protocol_code, ProjectCode=project_code, DistanceFromStart=effort_distance_km, SurveyAreaSize=effort_area_ha, NumberOfObservers=number_observers, AllIndividualsReported=all_species_reported, Remarks=trip_comments, Remarks2=species_comments, DecimalLatitude= latitude, DecimalLongitude=longitude, Locality=locality ) #Assign species_id and SpeciesCode ebird_data&lt;-merge(ebird_data, sp, by.x=&quot;CommonName&quot;, by.y=&quot;english_name&quot;) #Some species may be lost in this merge. Looking for a way to fix this using the naturecounts package. ebird_data&lt;-ebird_data %&gt;% dplyr::rename(SpeciesCode=species_code) ebird_data$RouteIdentifier&lt;-ebird_data$Locality ebird_data$BasisOfRecord &lt;- &quot;Observation&quot; ebird_data$CollectionCode &lt;- &quot;EBIRD&quot; ebird_data$InstitutionCode&lt;-&quot;Cornell&quot; ebird_data$Continent &lt;-&quot;North America&quot; ebird_data$ProtocolURL&lt;- &quot;https://support.ebird.org/en/support/solutions/articles/48000950859-guide-to-ebird-protocols&quot; ebird_data&lt;-ebird_data %&gt;% mutate(DurationInHours = duration_minutes/60) #Now that we have specified all the data columns we can, we will create the BMDE standardized data table. #Identify the missing columns of data BMDE_col&lt;-unique(BMDE$local_name) missing&lt;-setdiff(BMDE_col, names(ebird_data)) ebird_data[missing]&lt;-&quot; &quot; ebird_data&lt;-ebird_data[BMDE_col] 2.8.8 CBC To access CBC data you must contact Audubon at cbcadmin@audubon.org. A online form will be send to you to complete. When completing this form, scope the data to the geographic and temporal scale you require. You will also be asked if you want the auxiliary data included, such as effort and weather. Select yes to all options so that you have a complete dataset to work with. You will receive the raw data compressed in a Box folder, which you can download and save to the Data directory associated with this project. You will also be given links to download the âcbc_field_definitions_2013.pdfâ, which details the information in each data column, and the âCBCEditorialCodes.pdfâ, which defines the modifiers used when recording species. Both of these files are saved in the Data directory of this project. The tables provided will include: CBC_Circle_Species_Report: the raw counts of each species seen in each circle in each year CBC_Effort_Many_Types: for each count circle in each year, this table details the mode of data collection (e.g., car, foot), distance traveled, and the number of hours the survey took. CBC_Effort_Summary_Report: for each count in each year, this table details additional types of data collection effort, including field versus feeder counters, min and max parties, feeder hours, nocturnal hours, and nocturnal distance. These data are often used for effort correction. CBC_Weather_Report: for each count circle in each year, this table details all th weather covarites collected during the survey. CBC_Count_History_Report: this table detail when each count circle was run and in which years. This can be used to zero-fill the data matrix. We will once again work with the sample dataset here which has the same formatting as the full dataset that you will receive through the online download. #load Species Data CBC&lt;-read.csv(&quot;Data/CBC_Circle_Species_sample.csv&quot;) #separate columns CBC&lt;-CBC %&gt;% separate(subnational_code, into=c(&quot;del&quot;, &quot;StateProvince&quot;), sep=&quot;-&quot;) %&gt;% select(-del) CBC&lt;-CBC %&gt;% separate(cnt_dt, into=c(&quot;date&quot;, &quot;del&quot;), sep=&quot; &quot;) %&gt;% select(-del) CBC&lt;-CBC %&gt;% separate(date, into=c(&quot;MonthCollected&quot;, &quot;DayCollected&quot;, &quot;YearCollected&quot;), sep=&quot;/&quot;) #rename columns #circle ID = RouteIdentifier CBC&lt;-CBC %&gt;% dplyr::rename(RouteIdentifier=abbrev, Locality=name, DecimalLatitude= latitude, DecimalLongitude=longitude, Country=country_code, CommonName = com_name, ScientificName=sci_name, ObservationCount = how_many) #load effort measurements CBC_eff&lt;-read.csv(&quot;Data/CBC_Effort_Many_Types_Report.csv&quot;) #convert from miles to km CBC_eff&lt;-CBC_eff %&gt;% mutate(dist_km = ifelse(distance_unit==&quot;Miles&quot;, distance*1.609344, distance)) %&gt;% select(-distance, -OID_) #create total distance travelled CBC_eff&lt;-CBC_eff %&gt;% group_by(abbrev, name, count_yr) %&gt;% summarize(dis_tot=sum(dist_km), hour_tot=sum(hours)) #rename columns CBC_eff&lt;-CBC_eff %&gt;% dplyr::rename(RouteIdentifier=abbrev, Locality=name, DurationInHours = hour_tot, DistanceFromStart= dis_tot) #join tables CBC&lt;-left_join(CBC, CBC_eff, by=c(&quot;RouteIdentifier&quot;, &quot;Locality&quot;, &quot;count_yr&quot;)) #Remove subspecies since this does not allow tables to link CBC$CommonName&lt;-gsub(&quot;\\\\s*\\\\([^\\\\)]+\\\\)&quot;, &quot;&quot;, as.character(CBC$CommonName)) #replace Thayer&#39;s Gull with Ivory Gull CBC&lt;-CBC %&gt;% mutate(CommonName = ifelse(CommonName == &quot;Thayer&#39;s Gull&quot;, &quot;Ivory Gull&quot;, CommonName)) CBC&lt;-merge(CBC, sp, by.x=&quot;CommonName&quot;, by.y=&quot;english_name&quot;) CBC&lt;-CBC %&gt;% select(-ScientificName) %&gt;% rename(ScientificName = scientific_name, SpeciesCode=species_code) CBC$BasisOfRecord &lt;- &quot;Observation&quot; CBC$CollectionCode &lt;- &quot;CBC&quot; CBC$Continent &lt;-&quot;North America&quot; CBC$ProtocolSpeciesTargeted &lt;- &quot;All birds&quot; CBC$InstitutionCode&lt;-&quot;Audubon&quot; CBC$ProtocolURL=&quot;https://www.audubon.org/conservation/science/christmas-bird-count&quot; #Now that we have specified all the data columns we can, we will create the BMDE standardized data table. #Identify the missing columns of data BMDE_col&lt;-unique(BMDE$local_name) missing&lt;-setdiff(BMDE_col, names(CBC)) CBC[missing]&lt;-&quot; &quot; CBC&lt;-CBC[BMDE_col] 2.8.9 All Data Combined Now that all the datasets are in the BMDE format, we can simply bind these together into a single dataset. dat&lt;-rbind(BCCWS, BCMA, PSSS, PSEMP, ebird_data, CBC) dat$YearCollected&lt;-as.numeric(dat$YearCollected) dat$MonthCollected&lt;-as.numeric(dat$MonthCollected) dat$DayCollected&lt;-as.numeric((dat$DayCollected)) dat$ObservationCount&lt;-as.numeric((dat$ObservationCount)) 2.8.10 Zero-filling dataframe Now that you have a complete dateframe you will notice that in the ObervationCount column there are no zero counts. This is because surveyors only count what is present at a site, as opposed to what is not present (i.e., zero counts). Properly zero-filling your dataframe is important for most research purposes. It tells you when a site was surveyed, but a species of interest was not detected. To property zero-fill a dataframe you will want to create an events matrix using the full dataset (i.e., one which covers the temporal and spatial scale of interest). This event matrix will tell us which sites were surveyed and when. Then we can use this matrix to add the zeros during a species specific analysis. We donât add all the zeros to the occurrence dataset, because this would get very large. Note: the creation of the events matrix assumes that at least one species was detected at each survey point. This assumption is nearly always true #create the events matrix event&lt;-NULL events&lt;-dat %&gt;% select(YearCollected, MonthCollected, DayCollected, DecimalLatitude, DecimalLongitude, RouteIdentifier) %&gt;% distinct() #example of how to use the events matrix to zero-fill for Surf Scoter SUSC&lt;-dat %&gt;% filter(SpeciesCode==&quot;SUSC&quot;) SUSC&lt;-left_join(events, SUSC, by=c(&quot;YearCollected&quot;, &quot;MonthCollected&quot;, &quot;DayCollected&quot;, &quot;DecimalLatitude&quot;, &quot;DecimalLongitude&quot;, &quot;RouteIdentifier&quot;)) species&lt;-&quot;SUSC&quot; SUSC&lt;-SUSC %&gt;% mutate(ObservationCount = replace(ObservationCount, is.na(ObservationCount), 0), SpeciesCode = species) 2.8.11 Checking and removing duplicates Because some people use eBird to submit there data for various programs, there is a potential for duplicates in the full dataset. The events matrix does not allow multiple observations/ reports on the same day at the same location (latitude &amp; longitude), so by merging your species specific table to the events matrix, you should eliminate duplicates. If you want to ensure there are no duplicates you can run the following sample code. distinct&lt;-dat %&gt;% distinct(YearCollected, MonthCollected, DayCollected, TimeCollected, DecimalLatitude, DecimalLongitude, SpeciesCode, ObservationCount, RouteIdentifier, Locality, .keep_all=TRUE) 2.8.12 Assigning survey period Winter surveys more than often straddle two calendar years (e.g., start in October 2021 and end in April 2022). When doing an analysis you are often interested in âyearâ as a covarite or random effect. We will call this data column survey Period, and will assign this the start year of the surveys. NOTE: The PSEMP dataset contains the SurveyYear which is the end year of the survey period. We removed this during data processing but will add it back here. dat$Period &lt;- ifelse(dat$MonthCollected %in% c(8:12), dat$YearCollected, dat$YearCollected-1) 2.8.13 Remove out of range species/ unsuitable habitat Some surveys, like the CBC and eBird, are not specifically targeted towards sea ducks or other species of interest. We will therefore have surveys that are not of value to our assessment. There are several ways to filter these data, but what I find easy is identifying survey locations where an individual is not repeatably identified (e.g., &lt;=1), and remove these from the events list used for zero-filling. NOTE: I would work this into your data processing loop if doing this for many species Letâs use SUSC again as an example. site.summ &lt;- melt(dat, id.var = c(&quot;RouteIdentifier&quot;, &quot;SpeciesCode&quot;), measure.var = &quot;ObservationCount&quot;) site.summ &lt;- cast(site.summ, RouteIdentifier + SpeciesCode ~ variable, sum) site.sp.list &lt;- unique(subset(site.summ, select = c(&quot;RouteIdentifier&quot;, &quot;SpeciesCode&quot;), ObservationCount &gt; 1)) site.SUSC.list&lt;-site.sp.list %&gt;% filter(SpeciesCode==&quot;SUSC&quot;) #this will be your &#39;range&#39; list for SUSC. You can used this to remove out of range or rare records. test&lt;-left_join(site.SUSC.list, SUSC, by=c(&quot;SpeciesCode&quot;, &quot;RouteIdentifier&quot;)) 2.9 Additional Avian Data Resources Dataset were excluded from individual summaries if they were determined to be out of scope for one or more of the following reasons: The data are suitable for our assessment but are not yet available to the public. The primary focus was on pelagic seabirds, not seaducks. These datasets were also often collected outside the Salish Sea (our study area). The dataset was historical, meaning it was not readily accessible and/or lacked the necessary metadata. The surveys were narrowly scoped and species-specific. 2.9.1 Canadian Pacific Vessel-Based Survey In 2020, Environment and Climate Change Canada - Canadian Wildlife Service (ECCC-CWS) started vessel-based line transect surveys to collect quantitative baseline data on pelagic seabirds (not necessarily focused on seaducks). These data are being collected to provide a fundamental resource for mapping the abundance and distribution of birds at sea. Surveys are to continue through 2024, after which the data should be made publicly available. The main contact for this dataset is Caroline.Fox@ec.gc.ca 2.9.2 Pelagic Seabird Atlas The Atlas of Pelagic Seabirds off the West Coast of Canada presents maps that display the distribution of 48 species of seabirds. The rationale for developing this atlas was the recognized need for a product that could assist with: coastal zone and conservation area planning; emergency response to environmental emergencies; and identification of areas of potential interactions between seabirds and anthropogenic activities. In addition, the data used to develop the document provides a baseline to compare with future seabird distributions in order to measure the impacts of shifts in composition, abundance and/or distribution of prey, and climatic and oceanographic changes. Seabird surveys were conducted aboard commercial and Canadian federal government âships-of-opportunityâ from 1982-1983 and 1991-2005 within the study area (45Â° N to 58Â° N and from the coast to 148Â° W). The average densities within 5âlatitude by 5â longitude grid cells are displayed seasonally. These data may be used to generate presence/absence and trends in and estimates of relative abundance. These data can also be used to examine patterns in temporal and spatial distribution. However, due to the opportunistic nature of the surveys, both in space and time, these data should not be used to determine absolute abundance. All survey metadata can be found here. 2.9.3 North Pacific Pelagic Seabird Database (NPPSD) The NPPSD was created in 2005 to consolidate data on the oceanic distribution of marine bird species in the North Pacific. Most of these data were collected using at-sea strip-transect surveys within defined areas and at known locations. The NPPSD also contains observations of other bird species and marine mammals. The original NPPSD combined data from 465 surveys conducted between 1973 and 2002, primarily in waters adjacent to Alaska. These surveys included 61,195 sample transects with location, environment, and metadata information, and the data were organized in a flat-file format. It contains observations of 20,098,635 birds and 365,227 marine mammals. The NPPSD database and is available on the USGS Alaska Science Center NPPSD web site. Supplementary materials include an updated set of standardized taxonomic codes, reference maps that show the spatial and temporal distribution of the survey efforts and a downloadable query tool. All survey metadata can be found here. 2.9.4 Coastal Observation and Seabird Survey Team (COASST) Established in 1999, The COASST is a citizen science project housed at the University of Washington and focused on the beach environment of the northeast Pacific. Over 4,500 participants on more than 450 beaches spanning four states have contributed directly to monitoring their local marine resources and ecosystem health. Data collection happens on the outer Pacific Coast, not within the Salish Sea. All survey data can be explored here 2.9.5 Regional Population Monitoring of the Marbled Murrelet (WDFW) The Washington Department of Fish and Wildlife monitors populations of Marble Murrelets. This monitoring plan is specifically designed to estimate marbled murrelet density, population size, and population trend in each of five geographic areas (conservation zones) between the northern tip of Washington state and San Francisco, California. Within each zone, the offshore boundary denoting the extent of the target population is defined. Observers in small boats followed a prescribed at-sea transect line and recorded perpendicular distances to all murrelets observed from mid-May to the end of July when murrelets detected on the water are most likely locally breeding birds. Distance sampling methods are used to compute density and population estimates for the target population each year. Details on the program and reports can be found here. 2.9.6 Cherry Point Aquatic and Fidalgo Bay Aquatic Reserve shore-based monitoring (DNR - WA) This study used shore-based marine bird point counts to determine current abundance and evaluate long-term changesin abundance of several marine bird species within the Washington Department of Natural Resourcesâ Cherry Point Aquatic Reserve (CPAR) and Fidalgo Bay Aquatic Reserve (FBAR). This project originated at the CPAR in 2013 and began as a pilot project using the same protocols at Fidalgo Bay in 2017. Continued monitoring will allow statistical tests for better determining the abundance trends of marine birds in the CPAR and FBAR. Learn more about Cherry Point and Fidalgo Bay Aquatic Reserves 2.9.7 Additional Historical Datasets Å½ydelis, R., W. S. Boyd, A. Breault, and T. M. Sullivan. 2005. Abundance and distribution of waterbirds on the west coast of Vancouver Island during spring 1999 and winter 2000. Technical Report Series No. 437. Canadian Wildlife Service, Pacific and Yukon Region, British Columbia. Six aerial surveys of waterbirds were conducted on the west coast of Vancouver Island during the spring of 1999 andwinnerinter of 2000. Abundance was estimated in 274 pre-determined shoreline-based transects, each associated with a unique marine ecological unit (eco-unit). The replicated surveys of individual shoreline transects were processed to determine: 1) distribution and abundance of waterbirds on the near-shore portion of the west coast of Vancouver Island, 2) waterbird densities across marine ecological units and 3) seasonal variability in waterbird distribution and abundance Anderson, E.M., Bower, J.L., Nysewander, D.R., Evenson, J.R. &amp; Lovvorn, J.R. 2009. Changes in avifaunal abundance in a heavily used wintering and migration site in Puget Sound, Washington, during 1966â2007. Marine Ornithology 37: 19â27. Washington Department of Game (WDGânow the WDFW) conducted aerial surveys of waterfowl in Padilla Bay and other sites in Puget Sound from 1953 until at least 1976. US Fish and Wildlife Service (USFWS) has conducted aerial surveys of marine birds in Grays Harbor and at 64 sites in Puget Sound. These surveys occurred irregularly in various months and years, with surveys conducted in Padilla Bay from October to March of 1977â1986 (22 surveys). These data were compiled by M.J. McMinn and W.H. Schaff of the Nisqually National Wildlife Refuge. Marine Ecosystems Analysis (MESA) Puget Sound surveys were conducted one to three times per month from January 1978 to December 1979 in the Strait of Juan de Fuca and northern Puget Sound (see Wahl et al. 1981). Western Washington University survey replicated many of the MESA shore-based surveys in northern Puget Sound from September to mid-May 2003â2006 (Bower 2003). Bower, J. L. 2009. Changes in marine bird abundance in the Salish Sea: 1975 to 2007. Marine Ornithology 37:9-17. Marine Ecosystems Analysis (MESA) Puget Sound Project conducted during 1978/79 (see Wahl et al.Â 1981 below). Puget Sound Ambient Monitoring Program Western Washington University (WWU) or WWU/MESA comparisonâreplicating as closely as possible the substantial MESA data sets based upon shoreline and ferry counts conducted monthly from September to May, in 2003/04 and again in 2004/05 (Bower 2003) Nysewander, D.R., J.R. Evenson, B.L. Murphie, and T.A. Cyra. 2005. Report of marine bird and mammal component, Puget Sound Ambient Monitoring Program, for July 1992 to December 1999 period [Unpublished report]. Olympia, WA: Washington State Department of Fish and Wildlife, Wildlife Management Program. 181 pp.Â Available at: https://wdfw.wa.gov/publications/01135 Both summer and winter aerial surveys were conducted each year between 1992 and 1999. These surveys sampled the entire marine shoreline of greater Puget Sound by two strata between 1993 and 1997: nearshore (&lt; 20 meters [m]) and offshore (&gt; 20 m). They annually covered 13% to 15% of the nearshore and 3% to 5% of the offshore marine waters in Puget Sound up to the Canadian border and out to the west entrance to the Strait of Juan de Fuca. Comparable to the nearly identical winter aerial transects conducted during both 1978-79 MESA and 1992-99 PSAMP efforts. Wal, T. R. 1981. Marine bird populations of the Strait of Juan de Fuca, Strait of Georgia, and adjacent waters in 1978 and 1979. National Oceanic and Atmospheric Administration; United States Environmental Protection Agency, Research and Development, Seattle, WA; Washington, D.C.; Springfield, VA. (known as the MESA surveys) Conducted from 1 January 1978 to 31 December 1979 in the Strait of Juan de Fuca north to the San Juan Islands and Point Roberts and west to Sidney, British Columbia. Major objectives were to determine the time of occurrence, distribution, abundance, and locations of important concentrations of marine birds. Data were obtained on breeding marine birds on 99 geographic units in American waters. Additional species-specific datasets are identified by the Puget Sound Partnership (2012) here "],["SD2.html", "Chapter 3 Spatial Data 3.1 Overview 3.2 Available Data 3.3 Working With Different Data Sets in R 3.4 References", " Chapter 3 Spatial Data .table tr:nth-child(even) { background: #eee; } .book .book-body .page-wrapper .page-inner section.normal .leaflet-bar a, .book .book-body .page-wrapper .page-inner section.normal .leaflet-bar a:hover { color: #000000; } 3.1 Overview Previous research has identified a suite of biophysical predictors important in predicting the distribution and habitat use of sea ducks and coastal birds (Table 1). However, likely owing to the international border dividing the Salish Sea, many of these environmental/biophysical layers have yet to be compiled or used for trans-boundary habitat modeling. Furthermore, there are numerous available data sets that have yet to be tested in predicting the distribution of sea ducks that could be important (Table 2). The purpose of this chapter is to give an overview of spatial data that can support international sea duck habitat models in the Salish Sea. At the end of this chapter the user will know: What spatial data is available for modeling in the Salish Sea Where to get the data The limitations of the data (e.g., spatial resolution and temporal scale) How to work with different types of data Table 1. Summary of biophysical predictors used in modeling sea duck distribution and habitat use. Covariate Type Citations Net Primary Productivity Aquatic Lamb et al.Â 2020; Rickbeil et al.Â 2014 Sea Surface Temperature Aquatic Lamb et al.Â 2020; Rickbeil et al.Â 2014; Zipkins et al.Â 2010 Salinity Aquatic Lamb et al.Â 2020 Sediments Aquatic Rickbeil et al.Â 2014 Distance to Nearest Shoreline Spatial Lamb et al.Â 2020 Bottom Depth (Bathymetry) Spatial Lamb et al.Â 2020; Zipkins et al.Â 2010 Slope/Ocean Floor Topography Spatial Lamb et al.Â 2020; Zipkins et al.Â 2010 Bay or Otherwise Spatial Zipkins et al.Â 2010 Climate During Migration Climate Zipkins et al.Â 2010 Monthly Winter Temperature Climate Rickbeil et al.Â 2014 Monthly Winter Precipitation Climate Rickbeil et al.Â 2014 Shore Zone Terrestrial Rickbeil et al.Â 2014 Latitude Spatial Lamb et al.Â 2020; Zipkins et al.Â 2010 Longitude Spatial Lamb et al.Â 2020 Spatial Autocorrelation Other Zipkins et al.Â 2010 Temporal Autocorrelation Other Zipkins et al.Â 2010 Offset or Covariate for Effort Other Zipkins et al.Â 2010; Michel et al.Â 2021 Table 2. Summary of potential biophysical predictors not previously tested in modeling sea duck distribution and habitat use. Covariate Type Sea Vessel Traffic Spatial Bottom Patches Spatial Distribution of Prey Spatial Aquatic Vegiation Spatial 3.2 Available Data Below you will find a collection of available data sets that could be used in sea duck habitat suitability analysis. The data sets provided here have been guided by previous literature and may not be comprehensive. When available newer or better data sets have been included to replace out of date sources. Some covariates in Table 1 have been excluded as they are specific to the research being conducted (latitude/longitude, autocorrelation, and effort). A suite of data sets that have not been previously tested have also been included here. Not all of these data sets are trans boundary in nature but have been included for special use cases and could be updated if and when a corresponding data set becomes available. 3.2.1 Net Primary Productivity Net Primary Productivity (NPP), the measure of energy or biomass accumulation by primary producers, influences the distribution of consumers at greater trophic levels. With the launch of ocean-observing satellites, reasonable estimates of ocean primary production have become available. Numerous models exist for determining NPP, however, the vertical general productive model (VGPM) is a common standard and is a function of chlorophyll, available light, and photosynthetic efficiency (OâMalley 2012). Alternatively, the concentration chlorophyll-a has been used as an index of NPP as it related to phytoplankton biomass (Moses et al.Â 2009). 3.2.1.1 Data Sources Dataset Format Unit Resolution (km2) Timescale Source Vertical General Productive Model HDF mgC m-2 day-1 100 Monthly or 8day: 2002-2021 OâMalley 2012 Chlorophyll-a netCDF (.nc) mg m-3 1 Daily: 2002-Present MODIS 3.2.1.2 Data Use Considerations Both datasets are freely available to download and use from their respective links in the table above. User sign up might be required. The VGPM has a few flaws that might limit its usability for habitat suitability modeling. Firstly, its resolution of approximately 10km by 10km is a bit coarse. Furthermore, the model is limited to calculations south of 49o N in December and 53o N in January. With the northern most point of the Salish Sea Bio-region being at 50.8o N the VGPM would not cover the full extent of the study area of interest. Chlorophyll-a data from MODIS on the other hand comes with better resolution and timescale, however, this comes at a cost of file size and the number of files which can be cumbersome to deal with. Furthermore, even though the data set has a daily temporal scale, cloud cover can lead to missing data. To overcome this issue one could use MODIS Level 3 data that provides averages at 8-days or monthly at a resolution of 4 km which could be problematic depending on the time scale you might be interested in. 3.2.2 Sea Surface Temperature Sea Surface Temperature (SST), can also influence the distribution of species based on their thermal needs (Lamb et al.Â 2020; Zipkins et al. 2010). 3.2.2.1 Data Sources Dataset Format Unit Resolution (km2) Timescale Source MODIS Aqua 11 um Day/Night Sea Surface Temp netCDF (.nc) C 1 Daily: 2002-Present MODIS AVHRR Pathfinder SST netCDF (.nc) C 16 Daily: 1981-Present NOAA/NASA AVHRR Pathï¬nder Program MUR-SST netCDF (.nc) C 1 Daily: 2003-Present PODAAC 3.2.2.2 Data Use Considerations MODIS, Pathfinder, and MUR-SST data are freely accessible to download at their respective links above. MODIS data comes in a higher resolution but might be more cumbersome to work with given there are two satellite passes per day resulting in large file downloads. Pathfinder data is ready to use immediately and has day and night averages. Furthermore, like Chlorophyll-a data from MODIS, daily data can often be incomplete depending on cloud cover in the region of interest on a given day. An alternative would be to use MODIS level 3 data which averages over 8 day or monthly periods but at the cost of a lower resolution of 4km. MUR-SST data from the Physical Oceanography Distributed Active Archive Center is possibly a good solution to overcome the problems with the first two data sources. This data set uses data from multiple sensors that have been calibrated with each other to provide SST data that is consistant in time and space. This allows for high spatial resolution without data-voids due to cloud contamination. 3.2.3 Salinity Salinity has been shown to influence sea duck distribution. Lamb et al. (2020) found that during the winter 4 species of sea duck selected positively for salinity. 3.2.3.1 Data Sources Dataset Format Unit Resolution (km2) Timescale Source SMAP Salinity V5.0 netCDF (.nc) g kg-1 1600/4900 8-Day: 2015-Present Remote Sensing Systems 3.2.3.2 Data Use Considerations The SMAP Salinity V5.0 provides global coverage of sea surface salinity at a temporal scale of 8 days and at scales of 40km or 70km resulting in very poor spatial resolution (1600 km2 and 4900 km2 respectively). Furthermore, the 70km product is considered the official product and is based on smoothing of the 40 km product which can be noisy. Given this it would be very difficult to use this layer in a meaningful way to model sea duck habitat usage in the Salish Sea. 3.2.4 Distance to Nearest Shoreline Lamb et al.Â (2020) identified distance to nearest shore as a predictor in sea duck distribution during winter, however, this differed among species. This covariate was included along with bottom slope and depth as a proxy for fine-scale variation in things like currents and eddies. 3.2.4.1 Data Sources Dataset Format Resolution (km) Timescale Source World Vector Shoreline Shapefile (.shp) NA Static Global Self-consistent Hierarchical High-Resolution Database Prototype Global Shoreline Data Shapefile (.shp) NA Static National Geospatial-Intelligence Agency 3.2.4.2 Data Use Considerations Both data sets are freely available for download at their respective links in the above table. Word Vector Shoreline (WVS) data is in the form of land polygons whereas the Prototype Global Shoreline (PGS) data comes in the form of lines. The PGS data is based on 2000 Landsat imagery (~30m resolution) which is finer than the WVS which at itâs finest scale is about ~2.5km. Both data sets are old with newer imagery only being used to fill in gaps. 3.2.5 Bottom Depth (Bathymetry) and Ocean Floor Topography (Slope) Bottom depth and topography have been shown to influence sea duck distribution likely due to a number of factors. Mostly because these species rely heavily on benthic organisms, such as mollusks, sea ducks will prefer shallower areas for foraging where they can reach the sea floor that is supportive of prey populations (Lamb et al.Â 2020; Zipkins et al.Â 2010). Bottom depth and topography may also act as proxies for finer scale processes such as currents and eddies (Lamb et al.Â 2020). 3.2.5.1 Data Sources Dataset Format Unit Resolution (km2) Timescale Geography Source 2-minute Gridded Global Relief Data, (ETOPO2) v2 geoTiff (.geotiff) or netCDF (.nc) m 0.2025 Static US/CAD National Centers for Environmental Information Coastal Relief Model US netCDF (.nc) m 0.0081 Static US National Centers for Environmental Information Canadian Hydrographic Service Non-Navigational (NONNA) Bathymetric Data geoTiff (.geotiff) and others m 0.1 or 1 Static CAD Canadian Hydrographic Service Salish Sea Bathymetry Basemap geoTiff (.geotiff) m 0.0081 Static US/CAD Salish Sea Atlas (Western Washington University) 3.2.5.2 Data Use Considerations All data sets are freely available for download using their respective links above. Ocean floor topography can be calculated using bathymetry and guidelines on how to do this will be covered in the data use guide below. Both the US Coastal Relief Model and the Canadian Hydrographic Service data are included for completeness, however, these data sets are not trans-boundary in nature and are at different spatial scales which limits their use for trans-boundary sea duck modeling. ETOPO2 bathymetry data is a global data set at a resolution of 15 Arc-Seconds (~450m) which would be a suitable solution to using different data sets from the US and Canada. Another alternative which is specific to the Salish Sea Bioregion is bathymetry data from Western Washington Universityâs Salish Sea Atlas. This data set was compiled using two different data sets from the US and Canada and is at a finer resolution than ETOPO2 data (3 Arc-seconds or ~90m). This data set is only suitable to use for work done within the Salish Sea Bioregion as the raster has been clipped to that boundary. 3.2.6 Climate Climatic variables such as the North Atlantic Oscillation (NAO) index has been shown to influence the distribution of wintering sea ducks (Zipkins et al.Â 2010). The NAO index is related to the climatic variability and has been shown to affect the marine environment and ultimately ecological processes in plants and animals. Positive values are associated with more winter storms of greater intensity. More specific to the pacific region is the Pacific Decadal Oscillation (PDO) which will be included here as it pertains to the Salish Sea. Rickbeil et al.Â (2014) found weak support for atmospheric variables (temperature and precipitation) predicting occurrence, however, these are included below for reference. 3.2.6.1 Data Sources Dataset Format Unit Resolution (km2) Timescale Source Pacific Decadal Oscillation (PDO) tabular NA NA Monthly: 1854-present National Centers for Environmental Information ClimateNA Multiple Multiple 0.64 Annual, Seasonal, Yearly Wang et al.Â 2016 3.2.6.2 Data Use Considerations All data sets are freely available using their respective links above. The PDO provides a single monthly index for the Pacific Ocean. ClimateNA on the other hand allows you to extract many monthly, seasonal, and annual climate variables (e.g Mean Temperature and Precipitation) from free point locations. This includes historical and future predicted values. The data can be accessed using the ClimateNA software, API, or R package. The major limitation to both of these data sources is that the finest temporal resolution is down to a given month. 3.2.7 Shore-zone Rickbeil et al.Â (2014) found that including more detailed shore-zone data from the Physical shore-zone mapping system for British Columbia provided modest improvements over only using remotely sensed environmental data for modeling the distributions of coastal bird species in BC. 3.2.7.1 Data Sources Dataset Format Unit Resolution (km2) Timescale Geography Source Physical shore-zone mapping system for British Columbia wms/kml Categorical Static CAD Howes et al 1997 Washington State ShoreZone Inventory geodatabase (.gdb) Categorical Static US Washington State Department of Natural Resources 3.2.7.2 Data Use Considerations Shore-zone mapping of both British Columbia and Washington was both done during the late 1990s using a comparable methodology based on Howes (1994). At the time of writing this the British Columbia shore-zone data is difficult to access and work with. Many links to download the data are broken leaving the only available data to be accessed via WMS or .kml. WMS is image based which makes extracting the data extremely difficult and the .kml option has known issues. Alternatively, the Washington data is easy to access in the form of a geodatabase at the link in the above table. Both data sets should be able to be used together (when available) but one important thing to note is that given their age they may not be representative of todayâs shore zone. 3.2.8 Sea Vessels 3.2.8.1 Data Sources Dataset Format Timescale Geography Source Vessel Traffic Routes shapefile (.shp) Static CAD/US Fisheries and Oceans Canada Shipping Fairways, Lanes, and Zones for US waters shapefile (.shp) Static US National Oceanic and Atmospheric Administration WSDOT - Ferry Routes shapefile (.shp) Static US Washington State Department of Transportation AIS Vessel Transit Counts geoTiff (.geotiff) 2015-2021 US/CAD U.S. Coast Guard 3.2.8.2 Data Use Considerations Both Canada and the US have data on sea vessel routes within the Salish Sea. The Canadian Vessel Traffic Routes data set contains information on ferry routes, mandatory direction of traffic flow, separation lines and zones. To be able to use this data in modeling sea duck distribution one would have to do a bit of research to understand what all of the different layers mean using the Canadian Chart 1 Symbols, Abbreviations and Terms. This is not as simple as using the Shipping Fairways, Lanes, and Zones for US waters which clearly shows shipping lanes in Washington. Washington State ferry routes have been included as they are not shown in the Shipping Fairway data set. The AIS Vessel Transit counts is an interesting data set that is primarily meant to be US but also spans into the Canadian side of the Salish Sea. This data set is a summary of each time a vessel track passes through a 100m grid cell. Ultimately showing areas of high or low traffic. Furthermore this can be broken down by vessel type: All, Cargo, Fishing, Passenger, Pleasure/Sailing, Tanker, and Tug/Tow. 3.2.9 Bottom Patches 3.2.9.1 Data Sources Dataset Format Unit Timescale Geography Source Nearshore Bottom Patches for Pacific Canada shapefile (.shp) Categorical Static CAD Fisheries and Oceans Canada 3.2.9.2 Data Use Considerations The nearshore bottom patches data set for the Pacific Coast of Canada provides continuous substrate map to a depth of 50m off the coast of BC. Substrate is a key indicator of habitat in this important ecosystem where data collection is challenging and expensive. Horizontal accuracy of this data ranges from meters to 10s of meters due to source data and data processing. Areas with higher data density are likely to be more accurate. Unfortunately, at this time there does not seem to be an analogous product for the southern Salish Sea. Habitat mapping has been done here but only in select locations that does not cover the entirety of the southern Salish Sea (see here). 3.2.10 Distribution of Prey Dataset Format Unit Timescale Geography Source Commercial Shellfish Harvest Sites shapefile (.shp) NA Current US Washington State Department of Health British Columbia Aquaculture CSV NA Current CAD Fisheries and Oceans Canada Pacific Herring spawn index CSV NA 1951-2022 CAD Fisheries and Oceans Canada Herring Spawning Locations shapefile (.shp) NA US Washington Dept of Fish and Wildlife Southern Salish Sea Herring Biomass Excel (.xlsx) Metric Tons 1973-2002 US Washington Dept of Fish and Wildlife 3.2.10.1 Data Use Considerations Aquaculture locations in British Columbia and Washington portions of the Salish Sea are both available from their respective links above. Data from Washington are provided in a spatial point layer whereas BC data is provided in a tabular format which contains latitude and longitude coordinates which can easily be converted to a spatial point layer as well. Both data sets simply provide spatial locations of aquaculture locations with the BC data set differing slightly as it includes other aquaculture locations in addition to shellfish. Only current locations of license holders are shown which do not appear to include historical locations. Fisheries and Oceans Canada provides a yearly Herring Spawn Index for approximately 300 sections along the coast of British Columbia. This is a relative index of Herring Spawn biomass. There is currently no analogous data set for the United States, however, actual biomass has been estimated by the Washington Department of Fish and Wildlife for the Puget Sound. Furthermore, for the same region there is a data set of spawning habitat although survey dates go as far back as 1969 and may not be representative today and there does not appear to be an equivalent data set for BC. 3.2.11 Aquatic Vegetation Dataset Format Timescale Geography Source British Columbia Marine Conservation Analysis shapefile (.shp) 2006-2013 CAD BC Conservation Foundation Eelgrass Extent - Coastal British Columbia shapefile (.shp) 2016 CAD Hakai Institute Washington State ShoreZone Inventory shapefile (.shp) 1994-2000 US Washington State Department of Natural Resources Washington Marine Vegetation Atlas geoJSON US Washington State Department of Natural Resources 3.2.11.1 Data Use Considerations The British Columbia Marine Conservation Analysis provides a suite of data on the aquatic vegetation in British Columbiaâs coastal waters. Available data includes information on Kelp, Surfgrass, Ditch Grass, and Eelgrass. Data sets vary and contain multiple sources per species (e.g. Eelgrass polygons, Priority Eelgrass Habitat, and Eelgrass Biobands). Metadata for each of these different data sets can be found here. A predicted data set representing polygons of Eelgrass for the BC Coast is available from the Hakai Institute. Currently data is not directly downloadable, however, a data request can be made to data@hakai.org. This data set uses coastline ShoreZone and bathymetry data to predict Eelgrass extent. More information can be found using the link in the above table. The Washington State ShoreZone Inventory contains data on aquatic vegetation for Washington state. Data from this survey is old coming from aerial video collected in 1994-2000. Information on Eelgrass, Dunegrass, Surfgrass, and Kelp is available in a linear format indication areas of continuous or patchy presence along the shore. The Washington Marine Vegetation Atlas provides spatial information on the vegetation that grows in nearshore areas for the southern Salish Sea. This data set consists of 2000 polygon areas that have been surveyed from 2 to 56 times and the presence or absence of different vegetation is reported (Seagrass, Kelp, and Macroalgae). Because this data only tells you presence absence information on abundance is not provided. Data is not easily downloadable, however, it can be queried or downloaded using R or GIS software. Below you will find instructions on accessing the data using R. 3.2.12 Additional Data Sources Below is a table with a list of additional data sources not covered above which may or may not be useful in modeling sea duck distribution in the Salish Sea. Each source contains a link where you can download and discover additional data and notes are provided to give an overview of what is available. Additional Data Source Notes Salish Sea Atlas The Salish Sea Atlas from Western Washington University is an open access digital book that contains cultural and environmental geospatial data sets for the Salish Sea Bioregion. Some data sets included are sea depth (mentioned above), landcover, total annual precipitaiton, and impervious surfaces. Bio-ORACLE Bio-ORACLE is a data source for GIS Rasters that provide global geophysical, biotic, and environmental data for marine surface and benthic areas. Data is proved at a resolution of approximately 9.2 km squares. Example data sets included are Chlorophyll concentration, current velocity, sea surface temperature, and salinity. Furthermore, data can be downloaded using the R package âsdmpredictors.â The British Columbia Marine Conservation Analysis (2006â2013) In addition to information on aquatic vegetation, the BCMCA contains a suite of additional data sets for BC coastal waters including tidal currents, Shorezone exposure, algae, invertebrates, birds, and mammals to name a few. It is important to note that data is &gt; 10 years old and is only available for BC. Washington Coastal Atlas In addition to information on aquatic vegetation, the Washington Coastal Atlas contains many other useful data sources specific to Shoreline, Ocean, Wetland, Administrative, and Land Cover. The interactive map allows you to view data and links to downloading (when available) are provided in layer information. BC Coastal Resource Information Management System The B.C. Coastal Resource Information Management System (CRIMS) is an interactive tool for viewing and downloading coastal and marine data for British Columbia. The development of this tool is ongoing meaning new layers can be added as they become available. Salish Sea Key Sites A derived product based on the Puget Sound Ambient Modeling Project data, expert opinion, and other sources. This identifies areas within the Salish Sea that are most important to sea ducks, and can be used as a baseline to be compared with current occurrence data of sea ducks. Analyses can update the key site atlas and identify additional or shifting key sites. More information can be found here 3.3 Working With Different Data Sets in R Working with multiple types of geospatial data sets like the ones mentioned above can be quite challenging especially if the user does not have access or a great understanding of software like ArcGIS/QGIS. Furthermore, doing any analysis using GIS software can be difficult to reproduce. R as a GIS tool has advanced considerably in the last 10 years and has the added benefit of being flexible, reproducible, and open source. In addition, many ecologists rely on R for statistical analysis of their data which makes a strong argument for using R for an entire analysis (data extraction, manipulation, and analysis). Here we will go over some useful ways to interact and extract data from the various types of data sets mentioned above to provide the user with a starting point for their analysis. 3.3.1 Getting Started To get started there are a number of useful packages worth mentioning that will be used in the following examples. Depending on the userâs skill set some of these packages may be familiar or completely new. The sf package provides simple feature access in R. This package works best for working with spatial data (point, line, polygon, multipolygon etc) associated with tabular attributes (e.g shapefiles). You may be familiar with the sp package that has similar functionality in a different format, however, this package is heading for retirement by the end of 2023 and does not support integration with tidyverse which is very popular among data scientists in R. The terra package is essentially a modern version of the raster package but faster and more flexible. Furthermore, the raster package currently relies on packages that are being depreciated along with sp at the end of the year. Download and then load the required packages: ### Install Packages #devtools::install_github(&#39;rstudio/leaflet&#39;) ### Use development version #install.packages(&quot;leaflet.extras&quot;) #install.packages(&quot;leaflet.providers&quot;) #install.packages(&quot;tidyverse&quot;) #install.packages(&quot;sf&quot;) #install.packages(&quot;terra&quot;) #install.packages(&quot;geojsonsf&quot;) #install.packages(&quot;exactextractr&quot;) ### Load required packages require(leaflet) require(leaflet.extras) require(leaflet.providers) require(tidyverse) require(sf) require(terra) require(geojsonsf) For the purpose of this tutorial we will focus specifically on data within the Salish Sea ecoregion. A copy of the shapefile can be downloaded from the Salish Sea Atlas or can simply be read into R using the geojsonsf package which allows for reading in spatial data directly from the web in geojson format. You can view an interactive map of the boundary using the leaflet and supporting packages. At time of writing this the CRAN version of leaflet was not working with the terra package. If you encounter this problem please download and use the development version devtools::install_github('rstudio/leaflet'). ### Read in Salish Sea Boundary from arcgis service SalishSeaBoundary&lt;-geojson_sf(&quot;https://services.arcgis.com/qboYD3ru0louQq4F/arcgis/rest/services/Salish_Sea_Bioregion_Boundary/FeatureServer/0/query?outFields=*&amp;where=1%3D1&amp;f=geojson&quot;) %&gt;% st_transform(4326) ### View interactive map of boundary leaflet(width = &quot;100%&quot;) %&gt;% addTiles() %&gt;% addPolygons(data=SalishSeaBoundary,color = &quot;black&quot;, weight = 2, smoothFactor = 1,opacity = 1.0, fillOpacity = 0.5, fillColor = &quot;darkgreen&quot;) %&gt;% addFullscreenControl() %&gt;% addLegend(colors = c(&quot;darkgreen&quot;),labels = c(&quot;Salish Sea Bioregion&quot;),position = &quot;bottomright&quot;) We are also going to need some pretend survey points for which we will use to extract spatial covariates. Imagine this data set is the location and abundance of Black Scoters during a given winter survey. We can read in the data and have a look at it here: ### Read in Salish Sea Boundary from arcgis service BLSC &lt;- read_sf(&quot;Data\\\\Points\\\\POINT.shp&quot;) ### View interactive map of boundary leaflet(width = &quot;100%&quot;) %&gt;% addTiles() %&gt;% addMarkers(data = BLSC,popup = ~paste0(&quot;Abundance: &quot;,as.character(Abundance))) %&gt;% addFullscreenControl() We might also want to summarize our survey area into a spatial grid rather than individual points for the purpose of modeling and predicting. This can easily be done using the sf package. To do this we need to make sure to we are using a coordinate system that is in the units we want. To make a 1km grid we can convert to UTM Zone 10 (26910) before creating the grid. We convert back to WGS84 simply for viewing on the map. ### Transform to UTM 10 make grid 1km in size and convert to spatial dataframe grid &lt;- BLSC %&gt;% st_transform(26910) %&gt;% st_make_grid(1000, what = &quot;polygons&quot;, square = FALSE) %&gt;% st_sf() %&gt;% st_transform(4326) ### View newly created grid leaflet(width = &quot;100%&quot;) %&gt;% addTiles() %&gt;% addPolygons(data=grid,color = &quot;black&quot;, weight = 1, smoothFactor = 1,opacity = 1.0, fillOpacity = 0) %&gt;% addMarkers(data = BLSC,popup = ~paste0(&quot;Abundance: &quot;,as.character(Abundance))) %&gt;% addFullscreenControl() Now that we have some starting data to play with we can begin to extract spatial covariates for them. 3.3.2 Working with NetCDF (.nc) Covariates Many of the data sets mentioned above come in netCDF format which is a very widely used format for storing and sharing scientific array data including raster data. To download data from some of the sources above you may need a set of coordinates to refine your search. This can be done using sf package to calculate the bounding box this can be done on your study area (SalishSeaBoundary) or based on the data you have (BLSC). Here is a link as an example of downloading MUR SST within the Salish Sea Ecoregion boundary. st_bbox(SalishSeaBoundary) ## xmin ymin xmax ymax ## -125.92876 46.59306 -120.65461 50.86204 st_bbox(BLSC) ## xmin ymin xmax ymax ## -123.04885 48.90451 -122.78620 49.04377 NetCDF data can be read using the terra package and easily visualized using leaflet. For exploration purposes the code below also includes a toggle to add the BLSC points and the grid we created. ### Read in Data NetCDF SST &lt;- terra::rast(&quot;Data\\\\jplMURSST41_862f_217f_f62b.nc&quot;) ### Set colour scale pal &lt;- colorNumeric(c(&quot;#0C2C84&quot;, &quot;#41B6C4&quot;, &quot;#FFFFCC&quot;), values(SST), na.color = &quot;transparent&quot;) ### Plot the data to view leaflet(width = &quot;100%&quot;) %&gt;% addTiles() %&gt;% addRasterImage(SST, colors = pal, opacity = 0.8, group = &quot;SST&quot;) %&gt;% addLegend(pal = pal, values = values(SST), title = &quot;Average Surface temp&quot;) %&gt;% addPolygons(data=grid,color = &quot;black&quot;, weight = 1, smoothFactor = 1,opacity = 1.0, fillOpacity = 0, group = &quot;Grid&quot;) %&gt;% addMarkers(data = BLSC,popup = ~paste0(&quot;Abundance: &quot;,as.character(Abundance)), group = &quot;BLSC&quot;) %&gt;% addFullscreenControl() %&gt;% addLayersControl( overlayGroups = c(&quot;SST&quot;, &quot;Grid&quot;,&quot;BLSC&quot;), options = layersControlOptions(collapsed = FALSE) ) %&gt;% hideGroup(&quot;Grid&quot;) Next we will want to extract these covariates and add them to our respective data sets. This can be done using the terra package and the extract() function. Points will just get the value attributed to them whereas polygons will require some sort of function to account for multiple raster values being within them. Typically for this scenario using the mean is appropriate. ### Do extraction for survey data BLSC &lt;- extract(SST,BLSC,bind=T) %&gt;% st_as_sf() grid &lt;- extract(SST,grid,fun=&quot;mean&quot;,bind=T) %&gt;% st_as_sf() ### See added column with data head(BLSC) ## Simple feature collection with 6 features and 2 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -123.0275 ymin: 48.98027 xmax: -122.8367 ymax: 49.04377 ## Geodetic CRS: WGS 84 ## Abundance analysed_sst geometry ## 1 44 7.298 POINT (-122.9805 49.01849) ## 2 86 7.287 POINT (-122.885 49.00976) ## 3 36 7.266 POINT (-123.0275 49.04377) ## 4 38 7.306 POINT (-122.8367 48.99122) ## 5 131 7.449 POINT (-122.9432 48.98027) ## 6 63 7.315 POINT (-122.9317 49.01492) head(grid) ## Simple feature collection with 6 features and 1 field ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -123.0625 ymin: 48.90728 xmax: -123.0488 ymax: 48.99558 ## Geodetic CRS: WGS 84 ## analysed_sst geometry ## 1 7.22900 POLYGON ((-123.0556 48.9072... ## 2 7.20900 POLYGON ((-123.0556 48.9228... ## 3 7.21125 POLYGON ((-123.0556 48.9384... ## 4 7.22850 POLYGON ((-123.0557 48.9540... ## 5 NaN POLYGON ((-123.0557 48.9696... ## 6 NaN POLYGON ((-123.0557 48.9851... 3.3.3 Working with Raster Covariates Raster data sets are another popular spatial format one comes across when working with spatial covariates. Typically these come in geoTiff format but other formats are supported such as Band Interleaved by Line (BIL), SAGA, ENVI, &amp; others. Just like NetCDF, geoTiff rasters are read into R the same way. For the purpose of this tutorial we will use the Bathymetry (bottom depth) raster compiled and supplied by the Salish Sea Atlas. ### Read in raster bathymetry and project to plot on leaflet map Depth &lt;- terra::rast(&quot;Data\\\\SS_Bathymetry\\\\SS_Bathymetry.tif&quot;) You can see what this raster file looks like below which was generated using the same leaflet code as we used for SST above. If using leaflet for plotting spatial data you may notice some irregularities due to different projections so sometimes you will have to project your data to WGS84 for visualization purposes like done below. For analysis purposes this projection may not be the best. Raster data can often be quite large and difficult to work with. The best way to go about this problem is to crop the raster to the extent of your study. This can easily be done using the crop() function from the terra package. We will use our grid we created to crop this raster. You can see in the map below that the data has been cropped according to the study area. ### crop raster to study area must be same crs Depth_Crop &lt;- terra::crop(Depth,grid %&gt;% st_transform(crs(Depth))) Much like SST we can get a set of values for our survey points and grid. You will notice a few transformations happening below. This is to make sure that the projection of our points and grids match the projection of the raster layer before being converted back to WGS84. ### Do extraction for survey data BLSC &lt;- extract(Depth_Crop,BLSC %&gt;% st_transform(crs(Depth_Crop)),bind=T) %&gt;% st_as_sf() %&gt;% st_transform(4326) grid &lt;- extract(Depth_Crop,grid %&gt;% st_transform(crs(Depth_Crop)),fun=&quot;mean&quot;,bind=T) %&gt;% st_as_sf() %&gt;% st_transform(4326) ### See added column with data head(BLSC) ## Simple feature collection with 6 features and 3 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -123.0275 ymin: 48.98027 xmax: -122.8367 ymax: 49.04377 ## Geodetic CRS: WGS 84 ## Abundance analysed_sst SS_Bathymetry geometry ## 1 44 7.298 -6.034532 POINT (-122.9805 49.01849) ## 2 86 7.287 -6.033193 POINT (-122.885 49.00976) ## 3 36 7.266 -1.037675 POINT (-123.0275 49.04377) ## 4 38 7.306 -16.794600 POINT (-122.8367 48.99122) ## 5 131 7.449 -29.292519 POINT (-122.9432 48.98027) ## 6 63 7.315 -6.834748 POINT (-122.9317 49.01492) head(grid) ## Simple feature collection with 6 features and 2 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -123.0625 ymin: 48.90728 xmax: -123.0488 ymax: 48.99558 ## Geodetic CRS: WGS 84 ## analysed_sst SS_Bathymetry geometry ## 1 7.22900 -132.48201 POLYGON ((-123.0556 48.9072... ## 2 7.20900 -130.37502 POLYGON ((-123.0556 48.9228... ## 3 7.21125 -115.75745 POLYGON ((-123.0556 48.9384... ## 4 7.22850 -48.45209 POLYGON ((-123.0557 48.9540... ## 5 NaN NaN POLYGON ((-123.0557 48.9696... ## 6 NaN NaN POLYGON ((-123.0557 48.9851... Bottom slope can also easily be calculated in R using the terrain() function within the terra package. You can read more about this function and the alternative metrics that can be calulated from terrain rasters here. Slope &lt;- terrain(Depth_Crop, v=&quot;slope&quot;, neighbors=8, unit=&quot;degrees&quot;) Slope values can then be extracted for our survey data just like did for depth. One important thing to note is that since we are working with slope which is in degrees the mean must be calculated differently. A function for doing so is provided below. Unfortunately, this function canât be used within the extract() function so averages have to be calculated manually for each grid polygon by extracting all slope values and averaging them using the custom function. #### mean_angle &lt;- function(a, angle=c(&quot;degree&quot;, &quot;radians&quot;)) { angle=angle[1] deg2rad &lt;- function(x) { x * pi/180} rad2deg &lt;- function(x) { x * 180/pi } deg2vec &lt;- function(x, ang = c(&quot;degree&quot;, &quot;radians&quot;)) { if(ang == &quot;degree&quot;) { a &lt;- c(sin(deg2rad(x)), cos(deg2rad(x))) } else if(ang == &quot;radians&quot;) { a &lt;- c(sin(x), cos(x)) } return(a) } vec2deg &lt;- function(x) { res &lt;- rad2deg(atan2(x[1], x[2])) if (res &lt; 0) { res &lt;- 360 + res } return(res) } mean_vec &lt;- function(x) { y &lt;- lapply(x, deg2vec, ang=angle) Reduce(`+`, y)/length(y) } return( vec2deg(mean_vec(a)) ) } ### Do extraction for survey data BLSC &lt;- extract(Slope,BLSC %&gt;% st_transform(crs(Slope)),bind=T) %&gt;% st_as_sf() %&gt;% st_transform(4326) ### Average slope more complicated must do this manually grid &lt;-grid %&gt;% mutate(AverageSlope=NA) for (i in 1:nrow(grid)) { gridvalues &lt;- extract(Slope,vect(grid[i,] %&gt;% st_transform(crs(Slope)))) if(!all(is.na(gridvalues$slope))){ grid$AverageSlope[i] &lt;- mean_angle(na.omit(gridvalues$slope))} else { grid$AverageSlope[i] &lt;- NA } } ### See added column with data head(BLSC) ## Simple feature collection with 6 features and 4 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -123.0275 ymin: 48.98027 xmax: -122.8367 ymax: 49.04377 ## Geodetic CRS: WGS 84 ## Abundance analysed_sst SS_Bathymetry slope geometry ## 1 44 7.298 -6.034532 0.023155068 POINT (-122.9805 49.01849) ## 2 86 7.287 -6.033193 0.261033239 POINT (-122.885 49.00976) ## 3 36 7.266 -1.037675 0.008198081 POINT (-123.0275 49.04377) ## 4 38 7.306 -16.794600 0.118249444 POINT (-122.8367 48.99122) ## 5 131 7.449 -29.292519 0.061369546 POINT (-122.9432 48.98027) ## 6 63 7.315 -6.834748 1.160786780 POINT (-122.9317 49.01492) head(grid) ## Simple feature collection with 6 features and 3 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -123.0625 ymin: 48.90728 xmax: -123.0488 ymax: 48.99558 ## Geodetic CRS: WGS 84 ## analysed_sst SS_Bathymetry geometry AverageSlope ## 1 7.22900 -132.48201 POLYGON ((-123.0556 48.9072... 0.4780118 ## 2 7.20900 -130.37502 POLYGON ((-123.0556 48.9228... 0.3352073 ## 3 7.21125 -115.75745 POLYGON ((-123.0556 48.9384... 1.1366872 ## 4 7.22850 -48.45209 POLYGON ((-123.0557 48.9540... 3.3713673 ## 5 NaN NaN POLYGON ((-123.0557 48.9696... 1.3949719 ## 6 NaN NaN POLYGON ((-123.0557 48.9851... NA The raster tools outlined above will also work for other raster layers such as the AIS Vessel Transit Counts data. 3.3.4 Working with Linear Covariates There are a number of linear covariates mentioned above that also require a bit of data manipulation or calculation in order to be useful. Specifically distance to nearest shoreline. Using the Prototype Global Shoreline Data mentioned above I will outline how a user would go about calculating this for both our BLSC points and our grids. You will need to download data for the appropriate region (region 26 in this case) and because it covers a larger area than needed lets clip it to the extent of the Salish Sea Bioregion. This can be done using the st_intersection() function from sf. Note that columns from both are normally included but I have just kept the columns from the original shoreline data. ### Read in shoreline data using sf Shoreline &lt;- read_sf(&quot;Data\\\\shapefile26\\\\cd26.shp&quot;) %&gt;% st_transform(4326) ### Clip shoreline data to just salish sea to make it smaller and only keep original columns Shoreline &lt;- st_intersection(Shoreline,SalishSeaBoundary) %&gt;% select(names(Shoreline)) Calculating the distance to nearest shoreline is probably more complicated than it needs to be but I will explain the steps. Because this data set comes as a bunch of linear features compared to a single line for shoreline we first need to subset the segments that are closest to each of our BLSC points. This can be done using the st_nearest_feature() function which basically gives us the index of the nearest feature for each point (some might be duplicated but that is okay). The next step is to get the nearest points between each pair of BLSC point and shoreline segment using the st_nearest_points() function. We can then at the same time calculate the distance between these two points using the st_length() function. ### Get nearest shoreline features for each point - should be 20 nearest &lt;- Shoreline %&gt;% slice(st_nearest_feature(BLSC,Shoreline)) ### Get distance in meters and make sure it is numeric distance &lt;- as.numeric(st_length(st_nearest_points(BLSC, nearest, pairwise = T))) ### Add distance to BLSC data BLSC &lt;- BLSC %&gt;% mutate(DistanceToShore=distance) Click on the points below to see their calculated distance to nearest shore. Use the added measurement tool on the map to test out the calculations (some variation will be expected). This process would also work for nearest distance to a ferry lane or other linear feature. As for calculations for the grid we created this can also be done easily and it would be recommended to do so to the centroid of each grid using the st_centroid() function. nearest &lt;- Shoreline %&gt;% slice(st_nearest_feature(st_centroid(grid),Shoreline)) ### Get distance in meters and make sure it is numeric distance &lt;- as.numeric(st_length(st_nearest_points(st_centroid(grid), nearest, pairwise = T))) ### Add distance to BLSC data grid &lt;- grid %&gt;% mutate(DistanceToShore=distance) Extracting values from other linear features would work very much the same way. For example, shoreline inventory data with information on shoreline type, one could extract information from that layer based on the nearest feature. I will give a quick example of how to do this using the shoreline data. Lets extract the ID of each shoreline segment. This could easily be any field such as shoreline type etc. ### Get nearest segment for each BLSC point nearest &lt;- Shoreline %&gt;% slice(st_nearest_feature(BLSC,Shoreline)) ### Get just the ID ShorelineID &lt;- nearest %&gt;% pull(ID) ### Add the ID to the BLSC data BLSC &lt;- BLSC %&gt;% mutate(ShorelineID=ShorelineID) 3.3.5 Climate Data Various climate data variables can be obtained using the ClimateNAr package. This package is not on CRAN so you will have to register for ClimateBC/NA and download and install the package as per there instructions. You can use this map to get the different types of inputs needed for getting climate variables for the specific period you want. Below is an example of how you could go about downloading seasonal climate variables for the BLSC data assuming this data was for the year 2022. ### Create column to store average winter temperature. This can be one or more of the values you are interested in. BLSC &lt;- BLSC %&gt;% mutate(Tave_wt=NA) ### Loop through each point and extract climate temp for (i in 1:nrow(BLSC)) { ### extract coordinates for each point coords &lt;- st_coordinates(BLSC[i,]) %&gt;% as.data.frame() ### Query climate NA for seasonal data for the year 2022 climate_vars &lt;- ClimateNA_API(ClimateBC_NA=&#39;NA&#39;, c(coords$Y,coords$X,0), period=&#39;Year_2022.ann&#39;, MSY=&#39;S&#39;) ### Populate value for that point. Can also do more than one value at a time. BLSC$Tave_wt[i] &lt;- climate_vars$Tave_wt } 3.3.6 Other Spatial Data There are many different types of data sets one might come across that may even be outside the ones listed here or gone over in more detail above. However, many of the tools mentioned above will still apply. For instance, instead of calculating the nearest distance to shore the same could be done on point data such as BC Aquaculture sites or distance to Eelgrass etc. Not all use cases have been explored here as there are endless ways to work with this type of data. Below are some additional resources that can be helpful for understanding spatial analysis in R. Introduction to Geospatial Raster and Vector Data with R Spatial Data Science with R and âterraâ Spatial Data Science 3.4 References Lamb JS, Paton PW, Osenkowski JE, Badzinski SS, Berlin AM, Bowman T, Dwyer C, Fara LJ, Gilliland SG, Kenow K. 2020. Assessing yearâround habitat use by migratory sea ducks in a multiâspecies context reveals seasonal variation in habitat selection and partitioning. Ecography. 43(12):1842â1858. Michel N. 2021. Avian Habitat Suitability Models for Puget Sound Estuary Birds. Prepared for the Puget Sound Ecosystem Monitoring Program, Puget Sound Partnership. Tacoma, WA. Rickbeil GJ, Coops NC, Drever MC, Nelson TA. 2014. Assessing coastal species distribution models through the integration of terrestrial, oceanic and atmospheric data. Journal of biogeography. 41(8):1614â1625. Zipkin EF, Gardner B, Gilbert AT, OâConnell AF, Royle JA, Silverman ED. 2010. Distribution patterns of wintering sea ducks in relation to the North Atlantic Oscillation and local environmental characteristics. Oecologia. 163:893â902. &gt;&gt;&gt;&gt;&gt;&gt;&gt; edebd789d7ea92f60591d5086ec0b09ade850ab7 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
